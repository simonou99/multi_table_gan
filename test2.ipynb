{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed277bcc-0b53-44b4-9072-343f704588ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6344e5-e74b-42a5-8a1d-95bd2f963a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import exrex\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from faker import Faker\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kstest, ks_2samp, chi2_contingency, wasserstein_distance, norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "from tensorflow.keras import layers, models, optimizers, losses, backend, constraints, initializers\n",
    "from sdv.metadata import MultiTableMetadata\n",
    "from rdt import HyperTransformer\n",
    "import rdt.transformers as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56aef810-48d7-4597-81df-f8024807d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker('en-NZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdcbabf-f624-4961-93c2-1232ffdeffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aa2d4-7b51-45ff-bc4a-1d3e9d7a0b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37644fb8-5f08-496c-8403-c221fb097027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collection(dir_path):\n",
    "    gtfs_files = [f for f in os.listdir(dir_path) if f.endswith('.csv') or f.endswith('.txt')]\n",
    "    data_collection = {}\n",
    "    for f in gtfs_files:\n",
    "        f_path = os.path.join(dir_path, f)\n",
    "        data = pd.read_csv(f_path)\n",
    "        if data.shape[0] > 1:\n",
    "            data_collection[f.split(\".\")[0]] = data\n",
    "    return data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43100845-8f4f-4fb9-b765-073cdfe09426",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_collection = load_data_collection(\"gtfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0112f797-2f74-4718-924b-dcffd5e225f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del real_data_collection['shapes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6873d-3247-47b8-951c-707a74764428",
   "metadata": {},
   "source": [
    "# Setup Metadata (i.e. Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ba27c-a971-4d46-999c-437e54c0a9e0",
   "metadata": {},
   "source": [
    "## (Optional) Adjust Daylight-Savings Delta-Time to Traditional DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea9aed0-2977-4dfd-a897-21c92a88499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time(df, datetime_cols):\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_timedelta(df[col])\n",
    "        df[col] = df[col] % pd.Timedelta(days=1)\n",
    "        df[col] = df[col].astype(str).str.split().str[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e71165-95db-4737-a4ea-9d37206e47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata = MultiTableMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c84b27c-7e53-4e1b-9201-48061c28203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['agency_fare_url', 'agency_email']\n",
    "real_data_collection['agency'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440ead5c-2455-41fe-be36-5f095a5650fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='agency', data=real_data_collection['agency'])\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_id',sdtype='id',regex_format='[A-Z]{2,5}')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_url',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_timezone',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_lang',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_phone',sdtype='categorical')\n",
    "sdv_metadata.set_primary_key(table_name='agency',column_name='agency_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52c4634-7c65-4eab-9ab8-5e99ac623b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='calendar',data=real_data_collection['calendar'])\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='monday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='tuesday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='wednesday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='thursday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='friday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='saturday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='sunday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='start_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='end_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.set_primary_key(table_name='calendar',column_name='service_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd1c41f-11fc-411b-b502-986d9845e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='calendar_dates',data=real_data_collection['calendar_dates'])\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='exception_type',sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73659a67-02b7-48f8-9ca9-80db23db46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['route_desc', 'route_url','route_sort_order']\n",
    "real_data_collection['routes'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a7596ec-4304-46e8-9d37-a61bae87f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='routes',data=real_data_collection['routes'])\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_id',sdtype='id',regex_format='[A-Z]{2,6}-\\d{3}')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='agency_id',sdtype='id',regex_format='[A-Z]{2,5}')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_short_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_long_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_color',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_text_color',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='contract_id',sdtype='id',regex_format='([A-Z]{1,5}|\\d{1,5})')\n",
    "sdv_metadata.set_primary_key(table_name='routes',column_name='route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233ab08a-2b87-47d3-a709-3e32deca8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['stop_desc', 'zone_id','stop_url','stop_timezone']\n",
    "real_data_collection['stops'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb901e83-76a8-4c04-a4ac-dce006a72066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='stops',data=real_data_collection['stops'])\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_id',sdtype='id',regex_format='\\d{4,5}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_code',sdtype='numerical',computer_representation='Int32')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_lat',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_lon',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='location_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='parent_station',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='platform_code',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='wheelchair_boarding',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='start_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='end_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.set_primary_key(table_name='stops',column_name='stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf79c7ea-e569-46c8-8c40-b4cd58655552",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_collection['stop_times'] = adjust_time(real_data_collection['stop_times'], ['arrival_time','departure_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2da9c5e3-9856-4253-bf85-ee7350a1a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='stop_times',data=real_data_collection['stop_times'])\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='trip_id',sdtype='id',regex_format='\\d{1,5}-\\d{1,7}-\\d{1,5}-2-[a-z\\d]{1,8}(-[a-z\\d]{1,8})?')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='arrival_time',sdtype='datetime',datetime_format='%H:%M:%S')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='departure_time',sdtype='datetime',datetime_format='%H:%M:%S')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_id',sdtype='id',regex_format='\\d{4,5}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_sequence',sdtype='numerical',computer_representation='Int32')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_headsign',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='pickup_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='drop_off_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='shape_dist_traveled',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='timepoint',sdtype='numerical',computer_representation='Int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3487b373-6712-48f7-9678-eba78f8c135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['trip_short_name', 'block_id']\n",
    "real_data_collection['trips'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "883d3e98-7c9c-416b-a452-ce22dd7edd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='trips',data=real_data_collection['trips'])\n",
    "sdv_metadata.update_column(table_name='trips',column_name='route_id',sdtype='id',regex_format='[A-Z]{2,6}-\\d{3}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='trip_id',sdtype='id',regex_format='\\d{1,5}-\\d{1,7}-\\d{1,5}-2-[a-z\\d]{1,8}(-[a-z\\d]{1,8})?')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='trip_headsign',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='direction_id',sdtype='id')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='shape_id',sdtype='id',regex_format='\\d{1,4}-\\d{5,6}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='wheelchair_accessible',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='bikes_allowed',sdtype='categorical')\n",
    "sdv_metadata.set_primary_key(table_name='trips',column_name='trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dd5647-bd82-446d-89b4-0d7dbf896dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='agency',\n",
    "    child_table_name='routes',\n",
    "    parent_primary_key='agency_id',\n",
    "    child_foreign_key='agency_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='calendar',\n",
    "    child_table_name='calendar_dates',\n",
    "    parent_primary_key='service_id',\n",
    "    child_foreign_key='service_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='calendar',\n",
    "    child_table_name='trips',\n",
    "    parent_primary_key='service_id',\n",
    "    child_foreign_key='service_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='routes',\n",
    "    child_table_name='trips',\n",
    "    parent_primary_key='route_id',\n",
    "    child_foreign_key='route_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='stops',\n",
    "    child_table_name='stop_times',\n",
    "    parent_primary_key='stop_id',\n",
    "    child_foreign_key='stop_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='trips',\n",
    "    child_table_name='stop_times',\n",
    "    parent_primary_key='trip_id',\n",
    "    child_foreign_key='trip_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "623366dc-f042-4b1f-add3-2d9a10714f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2866e-649d-4103-97eb-d7c4ce9652a3",
   "metadata": {},
   "source": [
    "## (Optional) View Inter-Table Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f057715-6360-4361-aa57-59d481a95003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.3 (20230416.2022)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"664pt\" height=\"1014pt\"\n",
       " viewBox=\"0.00 0.00 664.00 1014.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1010)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1010 660,-1010 660,4 -4,4\"/>\n",
       "<!-- agency -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>agency</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M25.5,-845.5C25.5,-845.5 184.5,-845.5 184.5,-845.5 190.5,-845.5 196.5,-851.5 196.5,-857.5 196.5,-857.5 196.5,-993.5 196.5,-993.5 196.5,-999.5 190.5,-1005.5 184.5,-1005.5 184.5,-1005.5 25.5,-1005.5 25.5,-1005.5 19.5,-1005.5 13.5,-999.5 13.5,-993.5 13.5,-993.5 13.5,-857.5 13.5,-857.5 13.5,-851.5 19.5,-845.5 25.5,-845.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-990.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"13.5,-982.5 196.5,-982.5\"/>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-967.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-952.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-937.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_url : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-922.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_timezone : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-907.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_lang : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-892.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_phone : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"13.5,-884.5 196.5,-884.5\"/>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-869.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: agency_id</text>\n",
       "</g>\n",
       "<!-- routes -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>routes</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M18,-589C18,-589 192,-589 192,-589 198,-589 204,-595 204,-601 204,-601 204,-766 204,-766 204,-772 198,-778 192,-778 192,-778 18,-778 18,-778 12,-778 6,-772 6,-766 6,-766 6,-601 6,-601 6,-595 12,-589 18,-589\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-762.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">routes</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"6,-755 204,-755\"/>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-739.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-724.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-709.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_short_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-694.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_long_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-679.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-664.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_color : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-649.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_text_color : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-634.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">contract_id : id</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"6,-627 204,-627\"/>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-611.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: route_id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-596.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (agency): agency_id</text>\n",
       "</g>\n",
       "<!-- agency&#45;&gt;routes -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>agency&#45;&gt;routes</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105,-845.58C105,-823.86 105,-800.09 105,-777.65\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.85,-774.5 115.35,-788 101.85,-781.5 108.85,-774.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-815.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;agency_id → agency_id</text>\n",
       "</g>\n",
       "<!-- calendar -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>calendar</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M339.5,-573.5C339.5,-573.5 464.5,-573.5 464.5,-573.5 470.5,-573.5 476.5,-579.5 476.5,-585.5 476.5,-585.5 476.5,-781.5 476.5,-781.5 476.5,-787.5 470.5,-793.5 464.5,-793.5 464.5,-793.5 339.5,-793.5 339.5,-793.5 333.5,-793.5 327.5,-787.5 327.5,-781.5 327.5,-781.5 327.5,-585.5 327.5,-585.5 327.5,-579.5 333.5,-573.5 339.5,-573.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"402\" y=\"-778.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">calendar</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"327.5,-770.5 476.5,-770.5\"/>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-755.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-740.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">monday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-725.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tuesday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-710.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wednesday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-695.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">thursday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-680.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">friday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-665.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">saturday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-650.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sunday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-635.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">start_date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-620.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">end_date : datetime</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"327.5,-612.5 476.5,-612.5\"/>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-597.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: service_id</text>\n",
       "</g>\n",
       "<!-- calendar_dates -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>calendar_dates</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M462,-347C462,-347 644,-347 644,-347 650,-347 656,-353 656,-359 656,-359 656,-449 656,-449 656,-455 650,-461 644,-461 644,-461 462,-461 462,-461 456,-461 450,-455 450,-449 450,-449 450,-359 450,-359 450,-353 456,-347 462,-347\"/>\n",
       "<text text-anchor=\"middle\" x=\"553\" y=\"-445.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">calendar_dates</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"450,-438 656,-438\"/>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-422.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-392.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">exception_type : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"450,-385 656,-385\"/>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-369.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-354.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (calendar): service_id</text>\n",
       "</g>\n",
       "<!-- calendar&#45;&gt;calendar_dates -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>calendar&#45;&gt;calendar_dates</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M461.25,-573.62C482.23,-535.05 504.98,-493.25 522.57,-460.92\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"526.43,-457.77 532.93,-471.27 519.43,-464.77 526.43,-457.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"549\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;service_id → service_id</text>\n",
       "</g>\n",
       "<!-- trips -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>trips</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-302C12,-302 198,-302 198,-302 204,-302 210,-308 210,-314 210,-314 210,-494 210,-494 210,-500 204,-506 198,-506 198,-506 12,-506 12,-506 6,-506 0,-500 0,-494 0,-494 0,-314 0,-314 0,-308 6,-302 12,-302\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-490.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trips</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-483 210,-483\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-467.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-452.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-437.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-422.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_headsign : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">direction_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-392.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">shape_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-377.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wheelchair_accessible : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-362.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">bikes_allowed : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-355 210,-355\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-339.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: trip_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-324.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (calendar): service_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-309.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (routes): route_id</text>\n",
       "</g>\n",
       "<!-- calendar&#45;&gt;trips -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>calendar&#45;&gt;trips</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.8,-619.74C294.06,-590.62 253.95,-555.16 202.13,-505.72\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205.98,-502.57 212.48,-516.07 198.98,-509.57 205.98,-502.57\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;service_id → service_id</text>\n",
       "</g>\n",
       "<!-- routes&#45;&gt;trips -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>routes&#45;&gt;trips</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105,-589.32C105,-562.4 105,-532.9 105,-505.55\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.85,-502.4 115.35,-515.9 101.85,-509.4 108.85,-502.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;route_id → route_id</text>\n",
       "</g>\n",
       "<!-- stops -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>stops</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M240.5,-286.5C240.5,-286.5 419.5,-286.5 419.5,-286.5 425.5,-286.5 431.5,-292.5 431.5,-298.5 431.5,-298.5 431.5,-509.5 431.5,-509.5 431.5,-515.5 425.5,-521.5 419.5,-521.5 419.5,-521.5 240.5,-521.5 240.5,-521.5 234.5,-521.5 228.5,-515.5 228.5,-509.5 228.5,-509.5 228.5,-298.5 228.5,-298.5 228.5,-292.5 234.5,-286.5 240.5,-286.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"330\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stops</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-498.5 431.5,-498.5\"/>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-483.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-468.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_code : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-453.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-438.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_lat : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_lon : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-408.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">location_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-393.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">parent_station : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-378.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">platform_code : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wheelchair_boarding : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-348.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">start_date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-333.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">end_date : datetime</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-325.5 431.5,-325.5\"/>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-310.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: stop_id</text>\n",
       "</g>\n",
       "<!-- stop_times -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop_times</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M134.5,-0.5C134.5,-0.5 301.5,-0.5 301.5,-0.5 307.5,-0.5 313.5,-6.5 313.5,-12.5 313.5,-12.5 313.5,-222.5 313.5,-222.5 313.5,-228.5 307.5,-234.5 301.5,-234.5 301.5,-234.5 134.5,-234.5 134.5,-234.5 128.5,-234.5 122.5,-228.5 122.5,-222.5 122.5,-222.5 122.5,-12.5 122.5,-12.5 122.5,-6.5 128.5,-0.5 134.5,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"218\" y=\"-219.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_times</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"122.5,-211.5 313.5,-211.5\"/>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-196.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-181.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">arrival_time : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-166.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">departure_time : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-151.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-136.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_sequence : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-121.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_headsign : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-106.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pickup_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">drop_off_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-76.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">shape_dist_traveled : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">timepoint : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"122.5,-53.5 313.5,-53.5\"/>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-38.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (stops): stop_id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (trips): trip_id</text>\n",
       "</g>\n",
       "<!-- stops&#45;&gt;stop_times -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>stops&#45;&gt;stop_times</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M284.28,-286.87C277.45,-269.51 270.4,-251.61 263.57,-234.26\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"267.42,-231.11 273.92,-244.61 260.42,-238.11 267.42,-231.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-256.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;stop_id → stop_id</text>\n",
       "</g>\n",
       "<!-- trips&#45;&gt;stop_times -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>trips&#45;&gt;stop_times</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145,-302.3C153.76,-280.24 163.1,-256.72 172.07,-234.13\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"175.93,-230.98 182.43,-244.48 168.93,-237.98 175.93,-230.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-256.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;trip_id → trip_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x201645dc1c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdv_metadata.visualize(output_filepath='visualization/gtfs.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fc5a31d-d3e7-4ec0-8dc5-304147611d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebab86-128e-4eda-b4c8-20e6d00b2de3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cfb79-74a0-4320-88f9-10ddf15b613a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Modelling Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e52d36-a306-4990-8915-b725bb8f30bf",
   "metadata": {},
   "source": [
    "Setup dictionary for identifying the primary key, parent(s), child(ren) and foreign key(s) of each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efaf279b-69d9-4b0e-af63-0ee4ea0442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvmetadata_to_modelmetadata(sdv_metadata):\n",
    "    new_format = {}\n",
    "    for table_name, table_info in sdv_metadata[\"tables\"].items():\n",
    "        new_format[table_name] = {\n",
    "            \"primary_key\": table_info.get(\"primary_key\"),\n",
    "            \"parent\": {},\n",
    "            \"child\": {},\n",
    "            \"is_sequential\": False,\n",
    "            \"sort_order\":None,\n",
    "            \"additional_key\": None,\n",
    "        }\n",
    "    for relationship in sdv_metadata[\"relationships\"]:\n",
    "        parent_table = relationship[\"parent_table_name\"]\n",
    "        child_table = relationship[\"child_table_name\"]\n",
    "        foreign_key = relationship[\"child_foreign_key\"]\n",
    "        new_format[child_table][\"parent\"][parent_table] = foreign_key\n",
    "        new_format[parent_table][\"child\"][child_table] = foreign_key\n",
    "    return new_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "080b5bf6-2630-418d-9144-1f5ead370dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_metadata = sdvmetadata_to_modelmetadata(sdv_metadata.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5a967e-f225-4224-9986-691c40cac372",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_metadata['calendar_dates']['is_sequential']=True\n",
    "modelling_metadata['calendar_dates']['sort_order']=['service_id']\n",
    "modelling_metadata['stop_times']['is_sequential']=True\n",
    "modelling_metadata['stop_times']['sort_order']=['trip_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1096b3e-d02c-40e2-88d6-8cb7eec900da",
   "metadata": {},
   "source": [
    "## Fit RDT Transformers and Formatters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c4aac-ef2f-48c9-a776-26664cbb6075",
   "metadata": {},
   "source": [
    "Use tools from RDT package to convert data to solely numeric and back-transforming to the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a57b5bdd-e6cb-48e8-959e-02b1df2c36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_formatter_collection(data_collection):\n",
    "    formatter_collection = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        ht = HyperTransformer()\n",
    "        ht.detect_initial_config(df)\n",
    "        ht.update_transformers_by_sdtype(\n",
    "            sdtype='categorical',\n",
    "            transformer_name='LabelEncoder'\n",
    "        )\n",
    "        # ht.update_transformers_by_sdtype(\n",
    "        #     sdtype='datetime',\n",
    "        #     transformer_name='OptimizedTimestampEncoder'\n",
    "        # )\n",
    "        # ht.update_transformers_by_sdtype(\n",
    "        #     sdtype='numerical',\n",
    "        #     transformer_name='FloatFormatter'\n",
    "        # )\n",
    "        ht.fit(df)\n",
    "        formatter_collection[df_name] = ht\n",
    "    return formatter_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efcb8191-f71c-4011-a6da-38152ba1605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.77 s\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "formatter_collection = create_formatter_collection(real_data_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d0f92-95cd-4c87-af84-757da4eda60d",
   "metadata": {},
   "source": [
    "### (Optional) Adjust Incorrectly Detected Column Types from RDT's Auto-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a616b2d7-ae5c-4cda-a69b-8f25de8970b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"sdtypes\": {\n",
       "        \"trip_id\": \"categorical\",\n",
       "        \"arrival_time\": \"categorical\",\n",
       "        \"departure_time\": \"categorical\",\n",
       "        \"stop_id\": \"categorical\",\n",
       "        \"stop_sequence\": \"numerical\",\n",
       "        \"stop_headsign\": \"categorical\",\n",
       "        \"pickup_type\": \"numerical\",\n",
       "        \"drop_off_type\": \"numerical\",\n",
       "        \"shape_dist_traveled\": \"numerical\",\n",
       "        \"timepoint\": \"numerical\"\n",
       "    },\n",
       "    \"transformers\": {\n",
       "        \"trip_id\": LabelEncoder(),\n",
       "        \"arrival_time\": LabelEncoder(),\n",
       "        \"departure_time\": LabelEncoder(),\n",
       "        \"stop_id\": LabelEncoder(),\n",
       "        \"stop_sequence\": FloatFormatter(),\n",
       "        \"stop_headsign\": LabelEncoder(),\n",
       "        \"pickup_type\": FloatFormatter(),\n",
       "        \"drop_off_type\": FloatFormatter(),\n",
       "        \"shape_dist_traveled\": FloatFormatter(),\n",
       "        \"timepoint\": FloatFormatter()\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter_collection['stop_times'].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca1d08d0-9ba0-47bf-8321-95120758f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\rdt\\hyper_transformer.py:390: UserWarning: For this change to take effect, please refit your data using 'fit' or 'fit_transform'.\n",
      "  warnings.warn(self._REFIT_MESSAGE)\n",
      "C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\rdt\\hyper_transformer.py:299: UserWarning: For this change to take effect, please refit your data using 'fit' or 'fit_transform'.\n",
      "  warnings.warn(self._REFIT_MESSAGE)\n"
     ]
    }
   ],
   "source": [
    "formatter_collection['stop_times'].update_sdtypes(column_name_to_sdtype={\n",
    "    'pickup_type':'categorical',\n",
    "    'drop_off_type':'categorical'\n",
    "})\n",
    "formatter_collection['stop_times'].update_transformers_by_sdtype(\n",
    "    sdtype='categorical',\n",
    "    transformer_name='LabelEncoder'\n",
    ")\n",
    "formatter_collection['stop_times'].fit(real_data_collection['stop_times'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c2eb0-12ad-4146-a224-bf44c4811642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transform Data to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49560c4c-663c-4da6-a0f9-acabdd01deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transform(modelling_metadata, df_dict, ht_dict):\n",
    "    # Initialize the dictionary that will store the mappings for each primary key\n",
    "    primary_key_dict = {}\n",
    "\n",
    "    # Initialize a new dictionary to store the transformed dataframes\n",
    "    transformed_df_dict = {table: df.copy() for table, df in df_dict.items()}\n",
    "\n",
    "    # Initialize the dictionary to store transform methods\n",
    "    transform_method_dict = {table: {'labels': [], 'table': []} for table in df_dict.keys()}\n",
    "\n",
    "    # First pass: Fit the HT objects for all tables\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        ht = ht_dict[table_name]\n",
    "        ht.fit(transformed_df_dict[table_name])  # Fit the HT object on the entire table\n",
    "\n",
    "    # Second pass: factorize primary keys and build the global primary key dictionary\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        primary_key = table_info['primary_key']\n",
    "        if primary_key is not None:  # Skip tables without a primary key\n",
    "            transformed_df_dict[table_name][primary_key], unique = pd.factorize(transformed_df_dict[table_name][primary_key])\n",
    "            primary_key_dict[table_name] = {primary_key: {key: value for value, key in enumerate(unique)}}\n",
    "            transform_method_dict[table_name]['labels'].append(primary_key)\n",
    "\n",
    "    # Third pass: replace foreign keys using the primary key dictionary\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        for parent_table, foreign_key in table_info['parent'].items():\n",
    "            primary_key = modelling_metadata[parent_table]['primary_key']\n",
    "            transformed_df_dict[table_name][foreign_key] = transformed_df_dict[table_name][foreign_key].map(primary_key_dict[parent_table][primary_key])\n",
    "            transform_method_dict[table_name]['labels'].append(foreign_key)\n",
    "\n",
    "    # Fourth pass: transform the remaining columns with the fitted HT objects\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        ht = ht_dict[table_name]\n",
    "        df = transformed_df_dict[table_name]\n",
    "        remaining_cols = [col for col in df.columns if col not in transform_method_dict[table_name]['labels']]\n",
    "        if remaining_cols:\n",
    "            df[remaining_cols] = ht.transform_subset(df[remaining_cols])  # Transform only the remaining columns\n",
    "            transform_method_dict[table_name]['table'].extend(remaining_cols)\n",
    "\n",
    "    return transformed_df_dict, transform_method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b17dc196-89c7-4b9f-81db-03be45252196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.67 s\n",
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformed_data_collection, transform_method_dict = batch_transform(modelling_metadata, real_data_collection, formatter_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd532b97-d07d-4d17-b866-ed8c24e6a392",
   "metadata": {},
   "source": [
    "## Split Tables into Discrete and Continuous Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de5a1c05-f602-4318-b33e-63f33115d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(sdv_metadata, modelling_metadata):\n",
    "    # Define the discrete types.\n",
    "    discrete_sdtypes = ['categorical', 'id', 'boolean']\n",
    "\n",
    "    # Iterate over the metadata.\n",
    "    for df_name, content in sdv_metadata.items():\n",
    "        grouped_columns = {'discrete': [], 'continuous': []}\n",
    "        # Iterate over the columns of the DataFrame.\n",
    "        for column_name, col_type in content['columns'].items():\n",
    "            # If the column's 'sdtype' is one of the discrete types, add it to the 'discrete' group.\n",
    "            if col_type['sdtype'] in discrete_sdtypes:\n",
    "                grouped_columns['discrete'].append(column_name)\n",
    "            # elif col_type['sdtype'] == 'numerical':\n",
    "            #     if 'computer_representation' in col_type and 'Int' in col_type['computer_representation']:\n",
    "            #         grouped_columns['discrete'].append(column_name)\n",
    "            else:\n",
    "                # Otherwise, add it to the 'continuous' group.\n",
    "                grouped_columns['continuous'].append(column_name)\n",
    "        modelling_metadata[df_name]['numeric_types'] = grouped_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc1f7a13-f47f-4338-9171-878db4738413",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_columns(sdv_metadata.to_dict()['tables'], modelling_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cab2da9-db50-4db3-953d-cdc635c8bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_continuous_discrete(data_collection, modelling_metadata):\n",
    "    for df_name, content in modelling_metadata.items():\n",
    "        foreign_columns = []\n",
    "        label_columns = []\n",
    "        selected_columns = []\n",
    "        if content['parent'] != {}:\n",
    "            foreign_columns = list(modelling_metadata[df_name]['parent'].values())\n",
    "        discrete_columns = modelling_metadata[df_name]['numeric_types']['discrete']\n",
    "        if content['primary_key'] != None:\n",
    "            selected_columns = [col for col in discrete_columns if col != content['primary_key']]\n",
    "        selected_columns = list(set(foreign_columns) | set(selected_columns))\n",
    "        if selected_columns != [] or None:\n",
    "            modelling_metadata[df_name]['selected_columns'] = selected_columns\n",
    "        else:\n",
    "            modelling_metadata[df_name]['selected_columns'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f83047da-39a5-4831-8c52-76ab3dd5a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_continuous_discrete(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa2e80-2ca0-4d2c-881d-94de5c331119",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feb81dc5-44b4-4b54-9a6d-96a6b9fee048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_unique_elements(data):\n",
    "    unique_counts = data.nunique()\n",
    "    unique_dict = unique_counts.to_dict()\n",
    "    return unique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8846e997-fe62-44f4-9492-03ef527737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaling(df, min_=0, max_=1):\n",
    "    scaler = MinMaxScaler((min_,max_))\n",
    "    scaler = scaler.fit(df)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c29e184-ab47-4a05-9b1a-4f0ac24e8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_scaling(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(df)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0c8a567-b531-4b2c-b2ef-d50f188b9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_and_scaled_features(df, label_columns):\n",
    "    labels = df[label_columns]\n",
    "    table = df.drop(columns=label_columns)\n",
    "    table_column_names = table.columns\n",
    "    try:\n",
    "        scaled_table, table_scaler = normal_scaling(table)\n",
    "    except:\n",
    "        scaled_table = None\n",
    "        table_scaler = None\n",
    "    scaled_labels, labels_scaler = normal_scaling(labels)\n",
    "    return scaled_table, scaled_labels, table_scaler, labels_scaler, table_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c140cf3f-8d9e-4df9-b1cc-984d95a9806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_scaling(data_collection, modelling_metadata):\n",
    "    processed_data_collection = {}\n",
    "    backtransformation_dict = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        selected_columns = modelling_metadata[df_name]['selected_columns']\n",
    "        primary_key_column = modelling_metadata[df_name]['primary_key']\n",
    "        if primary_key_column != None:\n",
    "            label_columns = list(set([primary_key_column]) | set(selected_columns))\n",
    "        else:\n",
    "            label_columns = selected_columns\n",
    "            primary_key_scaler = None\n",
    "        table, labels, table_scaler, labels_scaler, table_columns = extract_key_and_scaled_features(df, label_columns)\n",
    "        processed_data_collection[df_name] = {'table':table,\n",
    "                                              'labels':labels}\n",
    "        backtransformation_dict[df_name] = {'table_scaler':table_scaler,\n",
    "                                            'labels_scaler':labels_scaler,\n",
    "                                       'table_col_names':table_columns,\n",
    "                                       'labels_col_names':label_columns,\n",
    "                                       'col_names':df.columns,\n",
    "                                           'unique_discrete_elements':n_unique_elements(df[selected_columns])}\n",
    "    return processed_data_collection, backtransformation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90c64e29-b4c5-4ea0-a6f3-3019fe5a7d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 312 ms\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data_collection, backtransformation_dict = batch_scaling(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235ddc3-4ce2-4f3b-9ebc-d04abc9e35c8",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa1b97-50ad-4e78-898b-dcab7c854a4a",
   "metadata": {},
   "source": [
    "## Fit KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13fc578c-be40-411e-b779-01160cb65a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kde(data, kernel='tophat', bandwidth='scott', leaf_size=40):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth, leaf_size=leaf_size)\n",
    "    kde.fit(data)\n",
    "    unique_elements_dict = n_unique_elements(data)\n",
    "    return kde, unique_elements_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b26696-67e0-4e28-a7b3-c927d67609e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_kde_fitting(data_collection, modelling_metadata):\n",
    "    kde_collection = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        selected_columns = modelling_metadata[df_name]['selected_columns']\n",
    "        if selected_columns != []:\n",
    "            kde, unique_elements = fit_kde(df[selected_columns])\n",
    "            kde_collection[df_name] = kde\n",
    "    return kde_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a08e6022-5106-4ef1-8559-10239f2bfe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 828 ms\n",
      "Wall time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kde_collection = batch_kde_fitting(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49eb54-1738-4599-874d-571ca5fdd9be",
   "metadata": {},
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744d496-c6f4-4c88-a505-3b82acc3304b",
   "metadata": {},
   "source": [
    "### Create Custom Functions for WGAN Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dce64b6b-045a-4764-9118-be6959932044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(x_fake):\n",
    "    return -tf.reduce_mean(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4af23f37-66e7-4aaa-9c91-ec955911a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(x_real, x_fake):\n",
    "    real_loss = tf.reduce_mean(x_real)\n",
    "    fake_loss = tf.reduce_mean(x_fake)\n",
    "    return fake_loss - real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5ab2a8f-c18a-47b2-bbc3-7b9b3337b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(batch_size, real_table, fake_table, real_labels, discriminator, timesteps=None):\n",
    "    if len(real_table.shape) == 2:\n",
    "        alpha = np.random.normal(0, 1, [batch_size, real_table.shape[1]])\n",
    "    else:\n",
    "        alpha = np.random.normal(0, 1, [batch_size, timesteps, real_table.shape[2]])\n",
    "    diff = fake_table - real_table\n",
    "    interpolated_table = real_table + alpha * diff\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated_table)\n",
    "        pred = discriminator([interpolated_table,real_labels], training=True)\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated_table])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25de12a5-2fc8-46cd-a2ea-cca99fa9ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeStep3D:\n",
    "    def __init__(self, n_steps):\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def transform(self, data):\n",
    "        X = np.array([data[i:i+self.n_steps] for i in range(len(data)-self.n_steps)])\n",
    "        return X.reshape((X.shape[0], X.shape[1], -1))  # reshape to 3D\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        # We only keep the last step of each sequence, as the rest is from previous sequences\n",
    "        return data[:,-1,:].reshape(-1, data.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64966e-d261-41f4-92b1-0d979c9deceb",
   "metadata": {},
   "source": [
    "### Define Plotting Function For G/D Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "558eca6b-dc6e-45a4-a77d-5a2139ef78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(logs):\n",
    "    n = len(logs)\n",
    "    n_rows = (n + 2) // 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (model_name, generator_log, discriminator_log) in enumerate(logs):\n",
    "        axes[i].plot(generator_log, label='gen')\n",
    "        axes[i].plot(discriminator_log, label='d')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(model_name)\n",
    "\n",
    "    for i in range(n, n_rows * 3):\n",
    "        axes[i].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134ba08-ad73-4d99-ae5d-a14e73c5a9d6",
   "metadata": {},
   "source": [
    "### Define Generator, discriminator and GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41167d3c-1509-42c7-bc7d-95694f11e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(table_dim, labels_dim, opt, loss, latent_dim=100):\n",
    "    noise_input = layers.Input(shape=(latent_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([noise_input, labels_input])\n",
    "    hidden_layer1 = layers.Dense(256,activation='relu',kernel_initializer='he_normal')(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    hidden_layer2 = layers.Dense(256,activation='relu',kernel_initializer='he_normal')(bn_layer1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    output_layer = layers.Dense(table_dim, activation='linear',kernel_initializer='glorot_normal')(bn_layer2)\n",
    "    generator = models.Model([noise_input, labels_input], output_layer)\n",
    "    generator.compile(optimizer=opt, loss=loss)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a95fea4d-2ef8-451c-99e1-9645b4f42429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(table_dim, labels_dim, opt, loss):\n",
    "    table_input = layers.Input(shape=(table_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([table_input, labels_input])\n",
    "    hidden_layer1 = layers.Dense(256,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal')(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    dropout1 = layers.Dropout(0.5)(bn_layer1)\n",
    "    hidden_layer2 = layers.Dense(256,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal')(dropout1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    dropout2 = layers.Dropout(0.5)(bn_layer2)\n",
    "    output_layer = layers.Dense(1, activation='linear',kernel_initializer='glorot_normal')(dropout2)\n",
    "    discriminator = models.Model([table_input, labels_input], output_layer)\n",
    "    discriminator.compile(optimizer=opt, loss=loss)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b5e0ad9-1df8-4406-91e5-6691a08a2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_generator(table_dim, labels_dim, timesteps, opt, loss, latent_dim=100):\n",
    "    noise_input = layers.Input(shape=(timesteps,latent_dim,))\n",
    "    labels_input = layers.Input(shape=(timesteps,labels_dim,))\n",
    "    merge_input = layers.Concatenate()([noise_input, labels_input])\n",
    "    hidden_layer1 = layers.Conv1DTranspose(256,kernel_size=4,strides=1,activation='relu',kernel_initializer='he_normal',padding='same',use_bias=False)(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    hidden_layer2 = layers.Conv1DTranspose(128,kernel_size=6,strides=2,activation='relu',kernel_initializer='he_normal',padding='same',use_bias=False)(bn_layer1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    hidden_layer3 = layers.Conv1DTranspose(64,kernel_size=8,strides=2,activation='relu',kernel_initializer='he_normal',padding='same',use_bias=False)(bn_layer2)\n",
    "    bn_layer3 = layers.BatchNormalization()(hidden_layer3)\n",
    "    hidden_layer4 = layers.Conv1DTranspose(32,kernel_size=10,strides=2,activation='relu',kernel_initializer='he_normal',padding='same',use_bias=False)(bn_layer3)\n",
    "    bn_layer4 = layers.BatchNormalization()(hidden_layer4)\n",
    "    flatten = layers.Reshape((1,timesteps,-1))(bn_layer4)\n",
    "    output_layer = layers.Dense(table_dim, activation='linear',kernel_initializer='glorot_normal')(flatten)\n",
    "    generator = models.Model([noise_input, labels_input], output_layer)\n",
    "    generator.compile(optimizer=opt, loss=loss)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b1374c1-e118-494f-8a44-bc8e93a0c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_discriminator(table_dim, labels_dim, timesteps, opt, loss):\n",
    "    table_input = layers.Input(shape=(timesteps,table_dim,))\n",
    "    labels_input = layers.Input(shape=(timesteps,labels_dim,))\n",
    "    merge_input = layers.Concatenate()([table_input, labels_input])\n",
    "    hidden_layer1 = layers.Conv1D(32,kernel_size=10,strides=2,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    dropout1 = layers.Dropout(0.2)(bn_layer1)\n",
    "    hidden_layer2 = layers.Conv1D(64,kernel_size=8,strides=2,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    dropout2 = layers.Dropout(0.2)(bn_layer2)\n",
    "    hidden_layer3 = layers.Conv1D(128,kernel_size=6,strides=2,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout2)\n",
    "    bn_layer3 = layers.BatchNormalization()(hidden_layer3)\n",
    "    dropout3 = layers.Dropout(0.2)(bn_layer3)\n",
    "    hidden_layer4 = layers.Conv1D(256,kernel_size=4,strides=1,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout3)\n",
    "    bn_layer4 = layers.BatchNormalization()(hidden_layer4)\n",
    "    dropout4 = layers.Dropout(0.2)(bn_layer4)\n",
    "    flatten = layers.Reshape((-1,timesteps,-1))(dropout4)\n",
    "    output_layer = layers.Dense(1, activation='linear',kernel_initializer='glorot_normal')(flatten)\n",
    "    discriminator = models.Model([table_input, labels_input], output_layer)\n",
    "    discriminator.compile(optimizer=opt, loss=loss)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cbc2d56-1baa-47de-8edf-aaf6b7ede7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_generator(table_dim, labels_dim, timesteps, opt, loss, latent_dim=100):\n",
    "    noise_input = layers.Input(shape=(timesteps,latent_dim,))\n",
    "    labels_input = layers.Input(shape=(timesteps,labels_dim,))\n",
    "    merge_input = layers.Concatenate()([noise_input, labels_input])\n",
    "    hiden_layer1 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal', return_sequences=True)(merge_input)\n",
    "    hiden_layer2 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal', return_sequences=True)(hiden_layer1)\n",
    "    hiden_layer3 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal', return_sequences=True)(hiden_layer2)\n",
    "    output_layer = layers.Dense(table_dim, activation='linear',kernel_initializer='glorot_normal')(hiden_layer3)\n",
    "    generator = models.Model([noise_input, labels_input], output_layer)\n",
    "    generator.compile(optimizer=opt, loss=loss)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52d62f5a-2c81-424d-ad94-680d07d463c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_lstm_discriminator(table_dim, labels_dim, timesteps, opt, loss):\n",
    "    table_input = layers.Input(shape=(timesteps,table_dim,))\n",
    "    labels_input = layers.Input(shape=(timesteps,labels_dim,))\n",
    "    merge_input = layers.Concatenate()([table_input, labels_input])\n",
    "    hiden_layer1 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal', return_sequences=True)(merge_input)\n",
    "    hiden_layer2 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal', return_sequences=True)(hiden_layer1)\n",
    "    hiden_layer3 = layers.GRU(64, activation='tanh',kernel_initializer='he_normal')(hiden_layer2)\n",
    "    output_layer = layers.Dense(1, activation='linear',kernel_initializer='glorot_normal')(hiden_layer3)\n",
    "    discriminator = models.Model([table_input, labels_input], output_layer)\n",
    "    discriminator.compile(optimizer=opt, loss=loss)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2b8d7-5798-490f-bf13-8233ef51b8d3",
   "metadata": {},
   "source": [
    "### Define Training Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c39aa00-6c56-4ad7-97d3-e6c3d98cb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_gan(name, real_table, real_labels, timesteps=6, neighbours=6, n_discriminator=5, gp_weight=10, epochs=50, batch_size=400, latent_dim=100, seed=123, return_logs=True):\n",
    "    rand_seed = seed\n",
    "    time_step_transformer = TimeStep3D(timesteps)\n",
    "    real_table = time_step_transformer.transform(real_table)\n",
    "    real_labels = time_step_transformer.transform(real_labels)\n",
    "    half_batch = int(batch_size/2)\n",
    "    g_opt = optimizers.Adam()\n",
    "    d_opt = optimizers.Adam()\n",
    "    g_loss_func = generator_loss\n",
    "    d_loss_func = discriminator_loss\n",
    "    generator = build_conv_generator(real_table.shape[2], real_labels.shape[2], timesteps=timesteps, opt=g_opt, loss=g_loss_func, latent_dim=latent_dim)\n",
    "    discriminator = build_conv_discriminator(real_table.shape[2], real_labels.shape[2], timesteps=timesteps, opt=d_opt, loss=d_loss_func)\n",
    "    y_real = -1* np.ones((half_batch, 1))\n",
    "    y_fake = np.ones((half_batch, 1))\n",
    "    y_gan = np.ones((batch_size, 1))\n",
    "    \n",
    "    if return_logs:\n",
    "        generator_logs = []\n",
    "        discriminator_logs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(rand_seed)\n",
    "        start_time = time.time()\n",
    "        d_loss = []\n",
    "        \n",
    "        # train discriminator:\n",
    "        for _ in range(n_discriminator):\n",
    "            idx = np.random.randint(0, real_table.shape[0], half_batch)\n",
    "            x_real_labels = real_labels[idx,:,:]\n",
    "            x_real_table = real_table[idx,:,:]\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = np.random.normal(0, 1, (half_batch, timesteps, latent_dim))\n",
    "                x_fake_table = generator([noise, x_real_labels], training=True)\n",
    "                d_logits_real = discriminator([x_real_table, x_real_labels], training=True)\n",
    "                d_logits_fake = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "                d_cost = d_loss_func(d_logits_real, d_logits_fake)\n",
    "                gp = gradient_penalty(half_batch, x_real_table, x_fake_table, x_real_labels, discriminator, timesteps=timesteps)\n",
    "                d_loss = d_cost + gp * gp_weight\n",
    "                rand_seed += 1\n",
    "                \n",
    "            d_grad = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            d_opt.apply_gradients(\n",
    "                zip(d_grad, discriminator.trainable_variables)\n",
    "            )\n",
    "            \n",
    "        d_loss = np.mean(d_loss)\n",
    "        # train generator/GAN\n",
    "        idx = np.random.randint(0, real_table.shape[0], batch_size)\n",
    "        x_real_labels = real_labels[idx,:,:]\n",
    "        x_gan = np.random.normal(0, 1, (batch_size, timesteps, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_fake_table = generator([x_gan, x_real_labels], training=True)\n",
    "            g_logits = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "            g_loss = g_loss_func(g_logits)\n",
    "        g_grad = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(\n",
    "            zip(g_grad, generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        if return_logs:\n",
    "            generator_logs.append(g_loss)\n",
    "            discriminator_logs.append(d_loss)\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        it_per_s = 1 / elapsed_time\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch in [0,epochs-1]:\n",
    "            print(f\"[Epoch: {epoch+1}] [C loss: {np.round(d_loss, 3)}] [G loss: {np.round(g_loss, 3)}] [Speed: {round(it_per_s, 2)} it/s]\")\n",
    "        \n",
    "        rand_seed += 1\n",
    "        \n",
    "    generator.save('test/'+name+'_gen.h5')\n",
    "    \n",
    "    if return_logs:\n",
    "        return generator_logs, discriminator_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b2a9050-72d7-4ce0-bc58-4906bb319af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_sequential_gan(name, real_table, real_labels, timesteps=6, n_discriminator=5, gp_weight=10, epochs=50, batch_size=400, latent_dim=100, seed=123, return_logs=True):\n",
    "    rand_seed = seed\n",
    "    time_step_transformer = TimeStep3D(timesteps)\n",
    "    real_table = time_step_transformer.transform(real_table)\n",
    "    real_labels = time_step_transformer.transform(real_labels)\n",
    "    half_batch = int(batch_size/2)\n",
    "    samp_size = min(batch_size, real_table.shape[0])\n",
    "    half_samp_size = min(half_batch, real_table.shape[0])\n",
    "    g_opt = optimizers.Adam()\n",
    "    d_opt = optimizers.Adam()\n",
    "    g_loss_func = generator_loss\n",
    "    d_loss_func = discriminator_loss\n",
    "    generator = build_conv_generator(real_table.shape[2], real_labels.shape[2], timesteps=timesteps, opt=g_opt, loss=g_loss_func, latent_dim=latent_dim)\n",
    "    discriminator = build_conv_discriminator(real_table.shape[2], real_labels.shape[2], timesteps=timesteps, opt=d_opt, loss=d_loss_func)\n",
    "    y_real = -1* np.ones((half_samp_size, 1))\n",
    "    y_fake = np.ones((half_samp_size, 1))\n",
    "    y_gan = np.ones((samp_size, 1))\n",
    "    \n",
    "    if return_logs:\n",
    "        generator_logs = []\n",
    "        discriminator_logs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(rand_seed)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train discriminator:\n",
    "        for _ in range(n_discriminator):\n",
    "            idx = np.random.randint(0, real_table.shape[0]-half_samp_size)\n",
    "            x_real_labels = real_labels[idx:(idx+half_samp_size),:,:]\n",
    "            x_real_table = real_table[idx:(idx+half_samp_size),:,:]\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = np.random.normal(0, 1, (half_samp_size, timesteps, latent_dim))\n",
    "                x_fake_table = generator([noise, x_real_labels], training=True)\n",
    "                d_logits_real = discriminator([x_real_table, x_real_labels], training=True)\n",
    "                d_logits_fake = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "                d_cost = d_loss_func(d_logits_real, d_logits_fake)\n",
    "                gp = gradient_penalty(half_samp_size, x_real_table, x_fake_table, x_real_labels, discriminator, timesteps=timesteps)\n",
    "                d_loss = d_cost + gp * gp_weight\n",
    "                rand_seed += 1\n",
    "                \n",
    "            d_grad = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            d_opt.apply_gradients(\n",
    "                zip(d_grad, discriminator.trainable_variables)\n",
    "            )\n",
    "            \n",
    "        d_loss = np.mean(d_loss)\n",
    "        # train generator/GAN\n",
    "        idx = np.random.randint(0, real_table.shape[0]-samp_size)\n",
    "        x_real_labels = real_labels[idx:(idx+samp_size),:,:]\n",
    "        x_gan = np.random.normal(0, 1, (samp_size, timesteps, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_fake_table = generator([x_gan, x_real_labels], training=True)\n",
    "            g_logits = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "            g_loss = g_loss_func(g_logits)\n",
    "        g_grad = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(\n",
    "            zip(g_grad, generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        if return_logs:\n",
    "            generator_logs.append(g_loss)\n",
    "            discriminator_logs.append(d_loss)\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        it_per_s = 1 / elapsed_time\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch in [0,epochs-1]:\n",
    "            print(f\"[Epoch: {epoch+1}] [C loss: {np.round(d_loss, 3)}] [G loss: {np.round(g_loss, 3)}] [Speed: {round(it_per_s, 2)} it/s]\")\n",
    "        \n",
    "        rand_seed += 1\n",
    "        \n",
    "    generator.save('test/'+name+'_gen.h5')\n",
    "    \n",
    "    if return_logs:\n",
    "        return generator_logs, discriminator_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ffe38c7-2d8e-4cfd-a7be-90b9f7804895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_training(processed_collection, modelling_metadata, training_parameter_dict):\n",
    "    logs = []\n",
    "    init_time = time.time()\n",
    "    for df_name, content in processed_collection.items():\n",
    "        start_time = time.time()\n",
    "        print(f'Learning \\\"{df_name}\\\" data......')\n",
    "        latent_dim = 100\n",
    "        if content['table'] is not None:\n",
    "            if modelling_metadata[df_name]['is_sequential'] == True:\n",
    "                generator_log, discriminator_log = train_individual_sequential_gan(\n",
    "                    df_name,\n",
    "                    content['table'],\n",
    "                    content['labels'],\n",
    "                    timesteps=6,\n",
    "                    epochs=training_parameter_dict[df_name]['epochs'],\n",
    "                    latent_dim = latent_dim,\n",
    "                    return_logs=True\n",
    "                )\n",
    "            else:\n",
    "                generator_log, discriminator_log = train_individual_gan(\n",
    "                    df_name,\n",
    "                    content['table'],\n",
    "                    content['labels'],\n",
    "                    epochs=training_parameter_dict[df_name]['epochs'],\n",
    "                    latent_dim = latent_dim,\n",
    "                    return_logs=True\n",
    "                )\n",
    "            logs.append([df_name, generator_log, discriminator_log])\n",
    "            in_loop_elapsed_time = time.time() - start_time\n",
    "            minutes, seconds = divmod(in_loop_elapsed_time, 60)\n",
    "            print(\"Time Used: %dm %ds\" % (minutes, seconds))\n",
    "            print('.')\n",
    "            print('.')\n",
    "        else:\n",
    "            print(\"Skipped (fully-discrete table, fitted with KDE instead)\")\n",
    "            print('.')\n",
    "            print('.')\n",
    "    plot_history(logs)\n",
    "    total_elapsed_time = time.time() - init_time\n",
    "    minutes, seconds = divmod(total_elapsed_time, 60)\n",
    "    print(\"Total Time Used: %dm %ds\" % (minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30c126-718f-4fb8-9e1f-788856cb0409",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c01837b4-de65-4540-815d-f34f47ef61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epochs(data_collection, batch_size=400):\n",
    "    parameter_dict = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        epochs = len(df) / batch_size\n",
    "        if epochs < 10:\n",
    "            epochs = 10\n",
    "        elif epochs > 500:\n",
    "            epochs = 500\n",
    "        parameter_dict[df_name] = {'epochs':round(epochs)}\n",
    "    return parameter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e617bab8-b5d8-4dcd-9bd7-b171c311dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameter_dict = calculate_epochs(transformed_data_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76243ed7-ea8b-46f0-9de0-e9891795f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_parameter_dict = {'agency': {'epochs': 10},\n",
    "#                            'calendar': {'epochs': 10},\n",
    "#                            'calendar_dates': {'epochs': 10},\n",
    "#                            'routes': {'epochs': 10},\n",
    "#                            'stops': {'epochs': 10},\n",
    "#                            'stop_times': {'epochs': 10},\n",
    "#                            'trips': {'epochs': 10}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d933f9ca-b23d-4235-ba11-d769163605c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency': {'epochs': 10},\n",
       " 'calendar': {'epochs': 10},\n",
       " 'calendar_dates': {'epochs': 10},\n",
       " 'routes': {'epochs': 10},\n",
       " 'stops': {'epochs': 17},\n",
       " 'stop_times': {'epochs': 500},\n",
       " 'trips': {'epochs': 81}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_parameter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "460d4f25-0081-4232-a6c0-1e520da7a6de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning \"agency\" data......\n",
      "Skipped (fully-discrete table, fitted with KDE instead)\n",
      ".\n",
      ".\n",
      "Learning \"calendar\" data......\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only specify one unknown dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[60], line 20\u001b[0m, in \u001b[0;36mcollection_training\u001b[1;34m(processed_collection, modelling_metadata, training_parameter_dict)\u001b[0m\n\u001b[0;32m     10\u001b[0m     generator_log, discriminator_log \u001b[38;5;241m=\u001b[39m train_individual_sequential_gan(\n\u001b[0;32m     11\u001b[0m         df_name,\n\u001b[0;32m     12\u001b[0m         content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m         return_logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     generator_log, discriminator_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_individual_gan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_parameter_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m logs\u001b[38;5;241m.\u001b[39mappend([df_name, generator_log, discriminator_log])\n\u001b[0;32m     29\u001b[0m in_loop_elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mtrain_individual_gan\u001b[1;34m(name, real_table, real_labels, timesteps, neighbours, n_discriminator, gp_weight, epochs, batch_size, latent_dim, seed, return_logs)\u001b[0m\n\u001b[0;32m      9\u001b[0m g_loss_func \u001b[38;5;241m=\u001b[39m generator_loss\n\u001b[0;32m     10\u001b[0m d_loss_func \u001b[38;5;241m=\u001b[39m discriminator_loss\n\u001b[1;32m---> 11\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_conv_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_loss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m build_conv_discriminator(real_table\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], real_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], timesteps\u001b[38;5;241m=\u001b[39mtimesteps, opt\u001b[38;5;241m=\u001b[39md_opt, loss\u001b[38;5;241m=\u001b[39md_loss_func)\n\u001b[0;32m     13\u001b[0m y_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones((half_batch, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[95], line 13\u001b[0m, in \u001b[0;36mbuild_conv_generator\u001b[1;34m(table_dim, labels_dim, timesteps, opt, loss, latent_dim)\u001b[0m\n\u001b[0;32m     11\u001b[0m hidden_layer4 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv1DTranspose(\u001b[38;5;241m32\u001b[39m,kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m'\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(bn_layer3)\n\u001b[0;32m     12\u001b[0m bn_layer4 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mBatchNormalization()(hidden_layer4)\n\u001b[1;32m---> 13\u001b[0m flatten \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbn_layer4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(table_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m,kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot_normal\u001b[39m\u001b[38;5;124m'\u001b[39m)(flatten)\n\u001b[0;32m     15\u001b[0m generator \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel([noise_input, labels_input], output_layer)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:996\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 996\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1134\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1132\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1133\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1139\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1140\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:867\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:907\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m    906\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m--> 907\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[0;32m    911\u001b[0m                         build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:561\u001b[0m, in \u001b[0;36mReshape.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    556\u001b[0m result \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    557\u001b[0m     inputs, (array_ops\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_shape)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    559\u001b[0m   \u001b[38;5;66;03m# Set the static shape for the result since it might lost during array_ops\u001b[39;00m\n\u001b[0;32m    560\u001b[0m   \u001b[38;5;66;03m# reshape, eg, some `None` dim in the result could be inferred.\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m   result\u001b[38;5;241m.\u001b[39mset_shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_output_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:551\u001b[0m, in \u001b[0;36mReshape.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m   output_shape \u001b[38;5;241m=\u001b[39m [input_shape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 551\u001b[0m   output_shape \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fix_unknown_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_shape\u001b[38;5;241m.\u001b[39mTensorShape(output_shape)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:530\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    528\u001b[0m     unknown \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m    529\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan only specify one unknown dimension.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m   known \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[1;31mValueError\u001b[0m: Can only specify one unknown dimension."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "collection_training(processed_data_collection, modelling_metadata, training_parameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08feb3fa-9912-4096-a489-4f6531b00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 0.0m 19.647764921188354s.\n"
     ]
    }
   ],
   "source": [
    "training_end_time = time.time()\n",
    "training_end_time = training_end_time - training_start_time\n",
    "minutes, seconds = divmod(training_end_time, 60)\n",
    "print(f'Total Training Time: {minutes}m {seconds}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7646e1f3-7104-4eeb-8e0d-8577f2fbc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37613df5-a0f7-4158-90e9-39c2f683bb49",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3df09-2fa3-43b2-b3ff-7e20bb6e5a78",
   "metadata": {},
   "source": [
    "## Set the number of rows to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e39be14-abfa-422e-a33a-f6333dc191a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_dict = {k:{'nrows':len(v['labels'])} for k,v in processed_data_collection.items()}\n",
    "generation_dict['calendar_dates']['sort_by'] = ['service_id','date']\n",
    "generation_dict['stop_times']['sort_by'] = ['trip_id','stop_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26712e2e-c0a2-4132-af93-68af04c1adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency': {'nrows': 15},\n",
       " 'calendar': {'nrows': 121},\n",
       " 'calendar_dates': {'nrows': 674, 'sort_by': ['service_id', 'date']},\n",
       " 'routes': {'nrows': 215},\n",
       " 'stops': {'nrows': 6714},\n",
       " 'stop_times': {'nrows': 966790, 'sort_by': ['trip_id', 'stop_sequence']},\n",
       " 'trips': {'nrows': 32403}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a43cfc-9f13-4762-b54d-56d1fd69ee0c",
   "metadata": {},
   "source": [
    "## Define Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e155a03a-d4ff-440c-bb49-e1b15a153097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_unique_elements(data, unique_elements, ignore=[]):\n",
    "    for col, n_unique in unique_elements.items():\n",
    "        if col not in ignore:\n",
    "            data[col] = np.round(data[col]).astype(int)\n",
    "            min_val, max_val = data[col].min(), data[col].max()\n",
    "            original_range = np.linspace(min_val, max_val, n_unique)\n",
    "            new_range = np.arange(n_unique)\n",
    "            # interp_func = interp1d(original_range, new_range, kind='linear', bounds_error=False, fill_value=(0, n_unique - 1))\n",
    "            interp_func = interp1d(original_range, new_range, kind='nearest', bounds_error=False, fill_value=(0, n_unique - 1))\n",
    "            data[col] = interp_func(data[col]).astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f65d8-b4b5-44cd-b5ed-65e355702a3d",
   "metadata": {},
   "source": [
    "## Define KDE Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f189dabd-19bf-4258-b1c0-ac076242a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(kde, n_samples, col_names, round_=True):\n",
    "    new_data = kde.sample(n_samples)\n",
    "    if round_:\n",
    "        new_data_df = pd.DataFrame(new_data, columns=col_names).round().astype(int)\n",
    "    else:\n",
    "        new_data_df = pd.DataFrame(new_data, columns=col_names)\n",
    "    return new_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771edf13-94eb-404e-b297-d967d61553d1",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7926ee0f-cac2-4ea9-9852-a36b9d86daa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_synth_data_collection(kde_collection, modelling_metadata, generation_dict, backtransformation_dict, formatter_collection, transform_method_dict, timesteps=6, latent_dim=100, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    synthetic_data_collection = {}\n",
    "    custom_objects = {'generator_loss':generator_loss}\n",
    "    for df_name, content in modelling_metadata.items():\n",
    "        if len(backtransformation_dict[df_name]['table_col_names']) != 0:\n",
    "            generator = models.load_model('test/'+df_name+'_gen.h5', custom_objects=custom_objects)\n",
    "        else:\n",
    "            generator = None\n",
    "        primary_key = content['primary_key']\n",
    "        child = content['child']\n",
    "        parent = content['parent']\n",
    "        n_samples = generation_dict[df_name]['nrows']\n",
    "        \n",
    "        fake_primary_key = None\n",
    "        fake_selected_columns = None\n",
    "        if primary_key != None:\n",
    "            fake_primary_key = np.arange(n_samples)\n",
    "            fake_primary_key = pd.DataFrame({primary_key:fake_primary_key})\n",
    "            \n",
    "        if content['selected_columns'] != []:\n",
    "            kde = kde_collection[df_name]\n",
    "            fake_selected_columns = generate_labels(kde, n_samples, content['selected_columns'])\n",
    "            unique_elements = backtransformation_dict[df_name]['unique_discrete_elements']\n",
    "            if parent != None:\n",
    "                ignore = []\n",
    "                for _, f_keys in parent.items():\n",
    "                    ignore.append(f_keys)\n",
    "                fake_selected_columns = adjust_unique_elements(fake_selected_columns, unique_elements, ignore=ignore)\n",
    "            else:\n",
    "                fake_selected_columns = adjust_unique_elements(fake_selected_columns, unique_elements)\n",
    "        \n",
    "        if fake_primary_key is not None:\n",
    "            if fake_selected_columns is not None:\n",
    "                fake_labels = pd.concat([fake_primary_key, fake_selected_columns], axis=1)\n",
    "            else:\n",
    "                fake_labels = fake_primary_key\n",
    "        else:\n",
    "            fake_labels = fake_selected_columns\n",
    "        \n",
    "        fake_labels = fake_labels.reindex(columns=backtransformation_dict[df_name]['labels_col_names'])\n",
    "        if content['is_sequential'] == True:\n",
    "            fake_labels = fake_labels.sort_values(by=content['sort_order'])\n",
    "            \n",
    "        time_step_transformer = TimeStep3D(timesteps)\n",
    "        scaled_fake_labels = backtransformation_dict[df_name]['labels_scaler'].transform(fake_labels)\n",
    "        scaled_fake_labels = time_step_transformer.transform(scaled_fake_labels)\n",
    "        \n",
    "        if generator is not None:\n",
    "            latent_dim = 100\n",
    "            noise = np.random.normal(0, 1, size=(n_samples-timesteps, timesteps, latent_dim))\n",
    "            raw_fake_table = generator.predict([noise, scaled_fake_labels])\n",
    "            raw_fake_table = time_step_transformer.inverse_transform(raw_fake_table)\n",
    "            fake_table = backtransformation_dict[df_name]['table_scaler'].inverse_transform(raw_fake_table)\n",
    "            fake_table = pd.DataFrame(fake_table, columns=backtransformation_dict[df_name]['table_col_names'])\n",
    "            fake_data = pd.concat([fake_labels, fake_table], axis=1)\n",
    "            fake_subset = formatter_collection[df_name].reverse_transform_subset(fake_data[transform_method_dict[df_name]['table']])\n",
    "            fake_data = pd.concat([fake_data[transform_method_dict[df_name]['labels']], fake_subset], axis=1)\n",
    "        else:\n",
    "            fake_data = fake_labels\n",
    "            fake_subset = formatter_collection[df_name].reverse_transform_subset(fake_data[transform_method_dict[df_name]['table']])\n",
    "            fake_data = pd.concat([fake_data[transform_method_dict[df_name]['labels']], fake_subset], axis=1)\n",
    "        fake_data = fake_data.reindex(columns=backtransformation_dict[df_name]['col_names'])\n",
    "        synthetic_data_collection[df_name] = fake_data\n",
    "        \n",
    "    return synthetic_data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1af7da14-2d7d-4c16-80e0-5c51f93f8675",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1584 predict_function  *\n        return step_function(self, iterator)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1574 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1567 run_step  **\n        outputs = model.predict_step(data)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1540 predict_step\n        return self(x, training=False)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1040 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 100), found shape=(None, 6, 100)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[72], line 52\u001b[0m, in \u001b[0;36mgenerate_synth_data_collection\u001b[1;34m(kde_collection, modelling_metadata, generation_dict, backtransformation_dict, formatter_collection, transform_method_dict, timesteps, latent_dim, seed)\u001b[0m\n\u001b[0;32m     50\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     51\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(n_samples\u001b[38;5;241m-\u001b[39mtimesteps, timesteps, latent_dim))\n\u001b[1;32m---> 52\u001b[0m raw_fake_table \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_fake_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m raw_fake_table \u001b[38;5;241m=\u001b[39m time_step_transformer\u001b[38;5;241m.\u001b[39minverse_transform(raw_fake_table)\n\u001b[0;32m     54\u001b[0m fake_table \u001b[38;5;241m=\u001b[39m backtransformation_dict[df_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39minverse_transform(raw_fake_table)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1749\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1748\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 1749\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1751\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:759\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3066\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3301\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3303\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:994\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    995\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1584 predict_function  *\n        return step_function(self, iterator)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1574 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1567 run_step  **\n        outputs = model.predict_step(data)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1540 predict_step\n        return self(x, training=False)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1040 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 100), found shape=(None, 6, 100)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "synthetic_data_collection = generate_synth_data_collection(kde_collection, modelling_metadata, generation_dict, backtransformation_dict, formatter_collection, transform_method_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32de55-988b-4271-9750-d5281462c68f",
   "metadata": {},
   "source": [
    "## Generate/Replace Key Columns with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56f7de02-e103-4384-b8ff-dc396dc8337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keys_with_regex(dataframes, metadata):\n",
    "    # Generate a mapping from old primary keys to new ones for each table\n",
    "    primary_key_mappings = {}\n",
    "    for table_name, table_info in metadata['tables'].items():\n",
    "        df = dataframes[table_name]\n",
    "\n",
    "        # Skip tables without a primary key\n",
    "        if 'primary_key' not in table_info:\n",
    "            continue\n",
    "\n",
    "        primary_key = table_info['primary_key']\n",
    "\n",
    "        # Skip columns without a regex_format\n",
    "        if 'regex_format' not in table_info['columns'][primary_key]:\n",
    "            continue\n",
    "\n",
    "        regex = table_info['columns'][primary_key]['regex_format']\n",
    "\n",
    "        # Generate new primary keys\n",
    "        new_primary_keys = [exrex.getone(regex) for _ in range(len(df))]\n",
    "\n",
    "        # Check if new keys are unique. If not, regenerate.\n",
    "        while len(new_primary_keys) != len(set(new_primary_keys)):\n",
    "            new_primary_keys = [exrex.getone(regex) for _ in range(len(df))]\n",
    "\n",
    "        # Create a mapping from old primary keys to new ones\n",
    "        primary_key_mappings[table_name] = dict(zip(df[primary_key], new_primary_keys))\n",
    "\n",
    "        # Replace the primary keys in the original dataframe\n",
    "        df[primary_key] = new_primary_keys\n",
    "\n",
    "    # Update the foreign keys in each table\n",
    "    for relationship in metadata['relationships']:\n",
    "        parent_table_name = relationship['parent_table_name']\n",
    "        child_table_name = relationship['child_table_name']\n",
    "        child_foreign_key = relationship['child_foreign_key']\n",
    "\n",
    "        # Get the mapping from old to new primary keys for the parent table\n",
    "        key_mapping = primary_key_mappings.get(parent_table_name)\n",
    "\n",
    "        # If there's no key mapping (i.e., the parent table has no primary key),\n",
    "        # skip this relationship\n",
    "        if key_mapping is None:\n",
    "            continue\n",
    "\n",
    "        # Replace the foreign keys in the child table\n",
    "        dataframes[child_table_name][child_foreign_key] = dataframes[child_table_name][child_foreign_key].map(key_mapping)\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f7bbba1-5552-4c7c-82f9-c22d64c0d0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_data_collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'synthetic_data_collection' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "synthetic_data_collection = replace_keys_with_regex(synthetic_data_collection, sdv_metadata.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8eb3b5d9-e177-480a-8ee7-d0658d9ad691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Generation Time: 0.0m 21.182543992996216s.\n"
     ]
    }
   ],
   "source": [
    "generation_end_time = time.time()\n",
    "generation_end_time = generation_end_time - training_start_time\n",
    "minutes, seconds = divmod(generation_end_time, 60)\n",
    "print(f'Total Generation Time: {minutes}m {seconds}s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dad6d-b2f2-4505-97c6-538b676f81c4",
   "metadata": {},
   "source": [
    "## (Optional) Adjust Daylight-Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "515a7451-06e9-48ad-b9a4-f6d39e8bc22f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_data_collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m synthetic_data_collection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_times\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m adjust_time(\u001b[43msynthetic_data_collection\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_times\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeparture_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'synthetic_data_collection' is not defined"
     ]
    }
   ],
   "source": [
    "synthetic_data_collection['stop_times'] = adjust_time(synthetic_data_collection['stop_times'], ['arrival_time','departure_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da7938-19e7-45a9-8b48-21c196f3c86b",
   "metadata": {},
   "source": [
    "# Save Generated Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640674b-5d1e-42e9-a9d8-52cf78c9c87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('test/real_data_collection.pkl', 'wb') as f:\n",
    "    pickle.dump(real_data_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622f869-9a16-4878-832c-2ea5f899e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test/synthetic_data_full_epoch.pkl', 'wb') as f:\n",
    "    pickle.dump(synthetic_data_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0559f9-d4b8-4244-98da-7877c32a72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test/sdv_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(sdv_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3303b-df2d-46ba-991a-2ad74a7d3220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
