{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed277bcc-0b53-44b4-9072-343f704588ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6344e5-e74b-42a5-8a1d-95bd2f963a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import exrex\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from faker import Faker\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kstest, ks_2samp, chi2_contingency, wasserstein_distance, norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "from tensorflow.keras import layers, models, optimizers, losses, backend, constraints, initializers\n",
    "from sdv.metadata import MultiTableMetadata\n",
    "from rdt import HyperTransformer\n",
    "import rdt.transformers as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56aef810-48d7-4597-81df-f8024807d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker('en-NZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdcbabf-f624-4961-93c2-1232ffdeffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aa2d4-7b51-45ff-bc4a-1d3e9d7a0b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37644fb8-5f08-496c-8403-c221fb097027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collection(dir_path):\n",
    "    gtfs_files = [f for f in os.listdir(dir_path) if f.endswith('.csv') or f.endswith('.txt')]\n",
    "    data_collection = {}\n",
    "    for f in gtfs_files:\n",
    "        f_path = os.path.join(dir_path, f)\n",
    "        data = pd.read_csv(f_path)\n",
    "        if data.shape[0] > 1:\n",
    "            data_collection[f.split(\".\")[0]] = data\n",
    "    return data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43100845-8f4f-4fb9-b765-073cdfe09426",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_collection = load_data_collection(\"gtfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0112f797-2f74-4718-924b-dcffd5e225f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del real_data_collection['shapes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea9aed0-2977-4dfd-a897-21c92a88499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time(df, datetime_cols):\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_timedelta(df[col])\n",
    "        df[col] = df[col] % pd.Timedelta(days=1)\n",
    "        df[col] = df[col].astype(str).str.split().str[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c84b27c-7e53-4e1b-9201-48061c28203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['agency_fare_url', 'agency_email']\n",
    "real_data_collection['agency'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73659a67-02b7-48f8-9ca9-80db23db46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['route_desc', 'route_url','route_sort_order']\n",
    "real_data_collection['routes'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233ab08a-2b87-47d3-a709-3e32deca8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['stop_desc', 'zone_id','stop_url','stop_timezone']\n",
    "real_data_collection['stops'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf79c7ea-e569-46c8-8c40-b4cd58655552",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_collection['stop_times'] = adjust_time(real_data_collection['stop_times'], ['arrival_time','departure_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3487b373-6712-48f7-9678-eba78f8c135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['trip_short_name', 'block_id']\n",
    "real_data_collection['trips'].drop(columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c4bb93-d3ef-43f2-8a89-9a32b171bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = [\n",
    "    {\n",
    "        \"parent_table_name\": \"agency\",\n",
    "        \"child_table_name\": \"routes\",\n",
    "        \"parent_primary_key\": \"agency_id\",\n",
    "        \"child_foreign_key\": \"agency_id\"\n",
    "    },\n",
    "    {\n",
    "        \"parent_table_name\": \"calendar\",\n",
    "        \"child_table_name\": \"calendar_dates\",\n",
    "        \"parent_primary_key\": \"service_id\",\n",
    "        \"child_foreign_key\": \"service_id\"\n",
    "    },\n",
    "    {\n",
    "        \"parent_table_name\": \"calendar\",\n",
    "        \"child_table_name\": \"trips\",\n",
    "        \"parent_primary_key\": \"service_id\",\n",
    "        \"child_foreign_key\": \"service_id\"\n",
    "    },\n",
    "    {\n",
    "        \"parent_table_name\": \"routes\",\n",
    "        \"child_table_name\": \"trips\",\n",
    "        \"parent_primary_key\": \"route_id\",\n",
    "        \"child_foreign_key\": \"route_id\"\n",
    "    },\n",
    "    {\n",
    "        \"parent_table_name\": \"stops\",\n",
    "        \"child_table_name\": \"stop_times\",\n",
    "        \"parent_primary_key\": \"stop_id\",\n",
    "        \"child_foreign_key\": \"stop_id\"\n",
    "    },\n",
    "    {\n",
    "        \"parent_table_name\": \"trips\",\n",
    "        \"child_table_name\": \"stop_times\",\n",
    "        \"parent_primary_key\": \"trip_id\",\n",
    "        \"child_foreign_key\": \"trip_id\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c809f80b-d3f8-4acd-87c3-492c06fbeaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_dataframes(dfs, relationships):\n",
    "    # sort relationships by parent table name\n",
    "    relationships = sorted(relationships, key=lambda x: x[\"parent_table_name\"])\n",
    "\n",
    "    # initialize a list of visited tables\n",
    "    visited = []\n",
    "\n",
    "    for rel in relationships:\n",
    "        parent_table = rel[\"parent_table_name\"]\n",
    "        child_table = rel[\"child_table_name\"]\n",
    "        parent_primary_key = rel[\"parent_primary_key\"]\n",
    "        child_foreign_key = rel[\"child_foreign_key\"]\n",
    "\n",
    "        # shrink parent dataframe if necessary and hasn't been visited yet\n",
    "        if parent_table not in visited and len(dfs[parent_table]) > 100:\n",
    "            dfs[parent_table] = dfs[parent_table].sample(n=100, random_state=1)\n",
    "            visited.append(parent_table)\n",
    "\n",
    "        # shrink child dataframe while maintaining relationship\n",
    "        if len(dfs[child_table]) > 100:\n",
    "            dfs[child_table] = pd.merge(\n",
    "                dfs[parent_table][[parent_primary_key]], \n",
    "                dfs[child_table], \n",
    "                how=\"inner\", \n",
    "                left_on=parent_primary_key, \n",
    "                right_on=child_foreign_key\n",
    "            )\n",
    "\n",
    "    # handle tables not in any relationship\n",
    "    for table in dfs.keys():\n",
    "        if table not in visited and len(dfs[table]) > 100:\n",
    "            dfs[table] = dfs[table].sample(n=100, random_state=1)\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d5be24-c37c-470e-a742-0e6a9b0a12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_collection = shrink_dataframes(real_data_collection, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6873d-3247-47b8-951c-707a74764428",
   "metadata": {},
   "source": [
    "# Setup Metadata (i.e. Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ba27c-a971-4d46-999c-437e54c0a9e0",
   "metadata": {},
   "source": [
    "## (Optional) Adjust Daylight-Savings Delta-Time to Traditional DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e71165-95db-4737-a4ea-9d37206e47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata = MultiTableMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440ead5c-2455-41fe-be36-5f095a5650fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='agency', data=real_data_collection['agency'])\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_id',sdtype='id',regex_format='[A-Z]{2,5}')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_url',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_timezone',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_lang',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='agency',column_name='agency_phone',sdtype='categorical')\n",
    "sdv_metadata.set_primary_key(table_name='agency',column_name='agency_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b52c4634-7c65-4eab-9ab8-5e99ac623b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='calendar',data=real_data_collection['calendar'])\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='monday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='tuesday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='wednesday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='thursday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='friday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='saturday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='sunday',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='start_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='calendar',column_name='end_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.set_primary_key(table_name='calendar',column_name='service_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fd1c41f-11fc-411b-b502-986d9845e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='calendar_dates',data=real_data_collection['calendar_dates'])\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='calendar_dates',column_name='exception_type',sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a7596ec-4304-46e8-9d37-a61bae87f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='routes',data=real_data_collection['routes'])\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_id',sdtype='id',regex_format='[A-Z]{2,6}-\\d{3}')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='agency_id',sdtype='id',regex_format='[A-Z]{2,5}')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_short_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_long_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_color',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='route_text_color',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='routes',column_name='contract_id',sdtype='id',regex_format='([A-Z]{1,5}|\\d{1,5})')\n",
    "sdv_metadata.set_primary_key(table_name='routes',column_name='route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb901e83-76a8-4c04-a4ac-dce006a72066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='stops',data=real_data_collection['stops'])\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_id',sdtype='id',regex_format='\\d{4,5}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_code',sdtype='numerical',computer_representation='Int32')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_name',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_lat',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='stop_lon',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='location_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='parent_station',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='platform_code',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='wheelchair_boarding',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='start_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.update_column(table_name='stops',column_name='end_date',sdtype='datetime',datetime_format='%Y%m%d')\n",
    "sdv_metadata.set_primary_key(table_name='stops',column_name='stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da9c5e3-9856-4253-bf85-ee7350a1a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='stop_times',data=real_data_collection['stop_times'])\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='trip_id',sdtype='id',regex_format='\\d{1,5}-\\d{1,7}-\\d{1,5}-2-[a-z\\d]{1,8}(-[a-z\\d]{1,8})?')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='arrival_time',sdtype='datetime',datetime_format='%H:%M:%S')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='departure_time',sdtype='datetime',datetime_format='%H:%M:%S')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_id',sdtype='id',regex_format='\\d{4,5}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_sequence',sdtype='numerical',computer_representation='Int32')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='stop_headsign',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='pickup_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='drop_off_type',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='shape_dist_traveled',sdtype='numerical',computer_representation='Float')\n",
    "sdv_metadata.update_column(table_name='stop_times',column_name='timepoint',sdtype='numerical',computer_representation='Int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "883d3e98-7c9c-416b-a452-ce22dd7edd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdv_metadata.detect_table_from_dataframe(table_name='trips',data=real_data_collection['trips'])\n",
    "sdv_metadata.update_column(table_name='trips',column_name='route_id',sdtype='id',regex_format='[A-Z]{2,6}-\\d{3}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='service_id',sdtype='id',regex_format='[a-zA-Z]{3,5}-\\d{1,2}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='trip_id',sdtype='id',regex_format='\\d{1,5}-\\d{1,7}-\\d{1,5}-2-[a-z\\d]{1,8}(-[a-z\\d]{1,8})?')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='trip_headsign',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='direction_id',sdtype='id')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='shape_id',sdtype='id',regex_format='\\d{1,4}-\\d{5,6}-[a-z\\d]{8}')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='wheelchair_accessible',sdtype='categorical')\n",
    "sdv_metadata.update_column(table_name='trips',column_name='bikes_allowed',sdtype='categorical')\n",
    "sdv_metadata.set_primary_key(table_name='trips',column_name='trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6dd5647-bd82-446d-89b4-0d7dbf896dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='agency',\n",
    "    child_table_name='routes',\n",
    "    parent_primary_key='agency_id',\n",
    "    child_foreign_key='agency_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='calendar',\n",
    "    child_table_name='calendar_dates',\n",
    "    parent_primary_key='service_id',\n",
    "    child_foreign_key='service_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='calendar',\n",
    "    child_table_name='trips',\n",
    "    parent_primary_key='service_id',\n",
    "    child_foreign_key='service_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='routes',\n",
    "    child_table_name='trips',\n",
    "    parent_primary_key='route_id',\n",
    "    child_foreign_key='route_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='stops',\n",
    "    child_table_name='stop_times',\n",
    "    parent_primary_key='stop_id',\n",
    "    child_foreign_key='stop_id'\n",
    ")\n",
    "\n",
    "sdv_metadata.add_relationship(\n",
    "    parent_table_name='trips',\n",
    "    child_table_name='stop_times',\n",
    "    parent_primary_key='trip_id',\n",
    "    child_foreign_key='trip_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "623366dc-f042-4b1f-add3-2d9a10714f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_metadata.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2866e-649d-4103-97eb-d7c4ce9652a3",
   "metadata": {},
   "source": [
    "## (Optional) View Inter-Table Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f057715-6360-4361-aa57-59d481a95003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.3 (20230416.2022)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"664pt\" height=\"1014pt\"\n",
       " viewBox=\"0.00 0.00 664.00 1014.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1010)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1010 660,-1010 660,4 -4,4\"/>\n",
       "<!-- agency -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>agency</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M25.5,-845.5C25.5,-845.5 184.5,-845.5 184.5,-845.5 190.5,-845.5 196.5,-851.5 196.5,-857.5 196.5,-857.5 196.5,-993.5 196.5,-993.5 196.5,-999.5 190.5,-1005.5 184.5,-1005.5 184.5,-1005.5 25.5,-1005.5 25.5,-1005.5 19.5,-1005.5 13.5,-999.5 13.5,-993.5 13.5,-993.5 13.5,-857.5 13.5,-857.5 13.5,-851.5 19.5,-845.5 25.5,-845.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-990.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"13.5,-982.5 196.5,-982.5\"/>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-967.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-952.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-937.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_url : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-922.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_timezone : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-907.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_lang : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-892.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_phone : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"13.5,-884.5 196.5,-884.5\"/>\n",
       "<text text-anchor=\"start\" x=\"21.5\" y=\"-869.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: agency_id</text>\n",
       "</g>\n",
       "<!-- routes -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>routes</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M18,-589C18,-589 192,-589 192,-589 198,-589 204,-595 204,-601 204,-601 204,-766 204,-766 204,-772 198,-778 192,-778 192,-778 18,-778 18,-778 12,-778 6,-772 6,-766 6,-766 6,-601 6,-601 6,-595 12,-589 18,-589\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-762.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">routes</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"6,-755 204,-755\"/>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-739.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">agency_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-724.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-709.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_short_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-694.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_long_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-679.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-664.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_color : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-649.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_text_color : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-634.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">contract_id : id</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"6,-627 204,-627\"/>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-611.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: route_id</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-596.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (agency): agency_id</text>\n",
       "</g>\n",
       "<!-- agency&#45;&gt;routes -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>agency&#45;&gt;routes</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105,-845.58C105,-823.86 105,-800.09 105,-777.65\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.85,-774.5 115.35,-788 101.85,-781.5 108.85,-774.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-815.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;agency_id → agency_id</text>\n",
       "</g>\n",
       "<!-- calendar -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>calendar</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M339.5,-573.5C339.5,-573.5 464.5,-573.5 464.5,-573.5 470.5,-573.5 476.5,-579.5 476.5,-585.5 476.5,-585.5 476.5,-781.5 476.5,-781.5 476.5,-787.5 470.5,-793.5 464.5,-793.5 464.5,-793.5 339.5,-793.5 339.5,-793.5 333.5,-793.5 327.5,-787.5 327.5,-781.5 327.5,-781.5 327.5,-585.5 327.5,-585.5 327.5,-579.5 333.5,-573.5 339.5,-573.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"402\" y=\"-778.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">calendar</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"327.5,-770.5 476.5,-770.5\"/>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-755.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-740.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">monday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-725.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tuesday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-710.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wednesday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-695.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">thursday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-680.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">friday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-665.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">saturday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-650.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sunday : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-635.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">start_date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-620.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">end_date : datetime</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"327.5,-612.5 476.5,-612.5\"/>\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-597.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: service_id</text>\n",
       "</g>\n",
       "<!-- calendar_dates -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>calendar_dates</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M462,-347C462,-347 644,-347 644,-347 650,-347 656,-353 656,-359 656,-359 656,-449 656,-449 656,-455 650,-461 644,-461 644,-461 462,-461 462,-461 456,-461 450,-455 450,-449 450,-449 450,-359 450,-359 450,-353 456,-347 462,-347\"/>\n",
       "<text text-anchor=\"middle\" x=\"553\" y=\"-445.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">calendar_dates</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"450,-438 656,-438\"/>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-422.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-392.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">exception_type : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"450,-385 656,-385\"/>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-369.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "<text text-anchor=\"start\" x=\"458\" y=\"-354.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (calendar): service_id</text>\n",
       "</g>\n",
       "<!-- calendar&#45;&gt;calendar_dates -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>calendar&#45;&gt;calendar_dates</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M461.25,-573.62C482.23,-535.05 504.98,-493.25 522.57,-460.92\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"526.43,-457.77 532.93,-471.27 519.43,-464.77 526.43,-457.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"549\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;service_id → service_id</text>\n",
       "</g>\n",
       "<!-- trips -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>trips</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-302C12,-302 198,-302 198,-302 204,-302 210,-308 210,-314 210,-314 210,-494 210,-494 210,-500 204,-506 198,-506 198,-506 12,-506 12,-506 6,-506 0,-500 0,-494 0,-494 0,-314 0,-314 0,-308 6,-302 12,-302\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-490.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trips</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-483 210,-483\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-467.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">route_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-452.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">service_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-437.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-422.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_headsign : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">direction_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-392.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">shape_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-377.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wheelchair_accessible : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-362.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">bikes_allowed : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-355 210,-355\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-339.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: trip_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-324.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (calendar): service_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-309.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (routes): route_id</text>\n",
       "</g>\n",
       "<!-- calendar&#45;&gt;trips -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>calendar&#45;&gt;trips</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.8,-619.74C294.06,-590.62 253.95,-555.16 202.13,-505.72\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205.98,-502.57 212.48,-516.07 198.98,-509.57 205.98,-502.57\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;service_id → service_id</text>\n",
       "</g>\n",
       "<!-- routes&#45;&gt;trips -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>routes&#45;&gt;trips</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105,-589.32C105,-562.4 105,-532.9 105,-505.55\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.85,-502.4 115.35,-515.9 101.85,-509.4 108.85,-502.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-543.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;route_id → route_id</text>\n",
       "</g>\n",
       "<!-- stops -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>stops</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M240.5,-286.5C240.5,-286.5 419.5,-286.5 419.5,-286.5 425.5,-286.5 431.5,-292.5 431.5,-298.5 431.5,-298.5 431.5,-509.5 431.5,-509.5 431.5,-515.5 425.5,-521.5 419.5,-521.5 419.5,-521.5 240.5,-521.5 240.5,-521.5 234.5,-521.5 228.5,-515.5 228.5,-509.5 228.5,-509.5 228.5,-298.5 228.5,-298.5 228.5,-292.5 234.5,-286.5 240.5,-286.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"330\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stops</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-498.5 431.5,-498.5\"/>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-483.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-468.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_code : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-453.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_name : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-438.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_lat : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_lon : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-408.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">location_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-393.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">parent_station : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-378.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">platform_code : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">wheelchair_boarding : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-348.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">start_date : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-333.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">end_date : datetime</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-325.5 431.5,-325.5\"/>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-310.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: stop_id</text>\n",
       "</g>\n",
       "<!-- stop_times -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop_times</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M134.5,-0.5C134.5,-0.5 301.5,-0.5 301.5,-0.5 307.5,-0.5 313.5,-6.5 313.5,-12.5 313.5,-12.5 313.5,-222.5 313.5,-222.5 313.5,-228.5 307.5,-234.5 301.5,-234.5 301.5,-234.5 134.5,-234.5 134.5,-234.5 128.5,-234.5 122.5,-228.5 122.5,-222.5 122.5,-222.5 122.5,-12.5 122.5,-12.5 122.5,-6.5 128.5,-0.5 134.5,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"218\" y=\"-219.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_times</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"122.5,-211.5 313.5,-211.5\"/>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-196.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">trip_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-181.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-166.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">arrival_time : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-151.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">departure_time : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-136.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_sequence : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-121.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">stop_headsign : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-106.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pickup_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">drop_off_type : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-76.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">shape_dist_traveled : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">timepoint : numerical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"122.5,-53.5 313.5,-53.5\"/>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-38.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (stops): stop_id</text>\n",
       "<text text-anchor=\"start\" x=\"130.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Foreign key (trips): trip_id</text>\n",
       "</g>\n",
       "<!-- stops&#45;&gt;stop_times -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>stops&#45;&gt;stop_times</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M284.28,-286.87C277.45,-269.51 270.4,-251.61 263.57,-234.26\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"267.42,-231.11 273.92,-244.61 260.42,-238.11 267.42,-231.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-256.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;stop_id → stop_id</text>\n",
       "</g>\n",
       "<!-- trips&#45;&gt;stop_times -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>trips&#45;&gt;stop_times</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145,-302.3C153.76,-280.24 163.1,-256.72 172.07,-234.13\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"175.93,-230.98 182.43,-244.48 168.93,-237.98 175.93,-230.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-256.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> &#160;trip_id → trip_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1551009f790>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdv_metadata.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebab86-128e-4eda-b4c8-20e6d00b2de3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cfb79-74a0-4320-88f9-10ddf15b613a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Modelling Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e52d36-a306-4990-8915-b725bb8f30bf",
   "metadata": {},
   "source": [
    "Setup dictionary for identifying the primary key, parent(s), child(ren) and foreign key(s) of each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efaf279b-69d9-4b0e-af63-0ee4ea0442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvmetadata_to_modelmetadata(sdv_metadata):\n",
    "    new_format = {}\n",
    "    for table_name, table_info in sdv_metadata[\"tables\"].items():\n",
    "        new_format[table_name] = {\n",
    "            \"primary_key\": table_info.get(\"primary_key\"),\n",
    "            \"parent\": {},\n",
    "            \"child\": {},\n",
    "            \"is_sequential\": False,\n",
    "            \"sort_order\":None,\n",
    "            \"additional_key\": None,\n",
    "        }\n",
    "    for relationship in sdv_metadata[\"relationships\"]:\n",
    "        parent_table = relationship[\"parent_table_name\"]\n",
    "        child_table = relationship[\"child_table_name\"]\n",
    "        foreign_key = relationship[\"child_foreign_key\"]\n",
    "        new_format[child_table][\"parent\"][parent_table] = foreign_key\n",
    "        new_format[parent_table][\"child\"][child_table] = foreign_key\n",
    "    return new_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "080b5bf6-2630-418d-9144-1f5ead370dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_metadata = sdvmetadata_to_modelmetadata(sdv_metadata.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da5a967e-f225-4224-9986-691c40cac372",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_metadata['calendar_dates']['is_sequential']=True\n",
    "modelling_metadata['calendar_dates']['sort_order']=['service_id']\n",
    "modelling_metadata['stop_times']['is_sequential']=True\n",
    "modelling_metadata['stop_times']['sort_order']=['trip_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1096b3e-d02c-40e2-88d6-8cb7eec900da",
   "metadata": {},
   "source": [
    "## Fit RDT Transformers and Formatters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c4aac-ef2f-48c9-a776-26664cbb6075",
   "metadata": {},
   "source": [
    "Use tools from RDT package to convert data to solely numeric and back-transforming to the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a57b5bdd-e6cb-48e8-959e-02b1df2c36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_formatter_collection(data_collection):\n",
    "    formatter_collection = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        ht = HyperTransformer()\n",
    "        ht.detect_initial_config(df)\n",
    "        ht.update_transformers_by_sdtype(\n",
    "            sdtype='categorical',\n",
    "            transformer_name='LabelEncoder'\n",
    "        )\n",
    "        # ht.update_transformers_by_sdtype(\n",
    "        #     sdtype='datetime',\n",
    "        #     transformer_name='OptimizedTimestampEncoder'\n",
    "        # )\n",
    "        # ht.update_transformers_by_sdtype(\n",
    "        #     sdtype='numerical',\n",
    "        #     transformer_name='FloatFormatter'\n",
    "        # )\n",
    "        ht.fit(df)\n",
    "        formatter_collection[df_name] = ht\n",
    "    return formatter_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efcb8191-f71c-4011-a6da-38152ba1605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 281 ms\n",
      "Wall time: 307 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "formatter_collection = create_formatter_collection(real_data_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d0f92-95cd-4c87-af84-757da4eda60d",
   "metadata": {},
   "source": [
    "### (Optional) Adjust Incorrectly Detected Column Types from RDT's Auto-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a616b2d7-ae5c-4cda-a69b-8f25de8970b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"sdtypes\": {\n",
       "        \"trip_id\": \"categorical\",\n",
       "        \"stop_id\": \"categorical\",\n",
       "        \"arrival_time\": \"categorical\",\n",
       "        \"departure_time\": \"categorical\",\n",
       "        \"stop_sequence\": \"numerical\",\n",
       "        \"stop_headsign\": \"categorical\",\n",
       "        \"pickup_type\": \"numerical\",\n",
       "        \"drop_off_type\": \"numerical\",\n",
       "        \"shape_dist_traveled\": \"numerical\",\n",
       "        \"timepoint\": \"numerical\"\n",
       "    },\n",
       "    \"transformers\": {\n",
       "        \"trip_id\": LabelEncoder(),\n",
       "        \"stop_id\": LabelEncoder(),\n",
       "        \"arrival_time\": LabelEncoder(),\n",
       "        \"departure_time\": LabelEncoder(),\n",
       "        \"stop_sequence\": FloatFormatter(),\n",
       "        \"stop_headsign\": LabelEncoder(),\n",
       "        \"pickup_type\": FloatFormatter(),\n",
       "        \"drop_off_type\": FloatFormatter(),\n",
       "        \"shape_dist_traveled\": FloatFormatter(),\n",
       "        \"timepoint\": FloatFormatter()\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter_collection['stop_times'].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca1d08d0-9ba0-47bf-8321-95120758f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\rdt\\hyper_transformer.py:390: UserWarning: For this change to take effect, please refit your data using 'fit' or 'fit_transform'.\n",
      "  warnings.warn(self._REFIT_MESSAGE)\n",
      "C:\\programs\\conda\\envs\\tfgpu\\lib\\site-packages\\rdt\\hyper_transformer.py:299: UserWarning: For this change to take effect, please refit your data using 'fit' or 'fit_transform'.\n",
      "  warnings.warn(self._REFIT_MESSAGE)\n"
     ]
    }
   ],
   "source": [
    "formatter_collection['stop_times'].update_sdtypes(column_name_to_sdtype={\n",
    "    'pickup_type':'categorical',\n",
    "    'drop_off_type':'categorical'\n",
    "})\n",
    "formatter_collection['stop_times'].update_transformers_by_sdtype(\n",
    "    sdtype='categorical',\n",
    "    transformer_name='LabelEncoder'\n",
    ")\n",
    "formatter_collection['stop_times'].fit(real_data_collection['stop_times'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c2eb0-12ad-4146-a224-bf44c4811642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transform Data to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49560c4c-663c-4da6-a0f9-acabdd01deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transform(modelling_metadata, df_dict, ht_dict):\n",
    "    # Initialize the dictionary that will store the mappings for each primary key\n",
    "    primary_key_dict = {}\n",
    "\n",
    "    # Initialize a new dictionary to store the transformed dataframes\n",
    "    transformed_df_dict = {table: df.copy() for table, df in df_dict.items()}\n",
    "\n",
    "    # Initialize the dictionary to store transform methods\n",
    "    transform_method_dict = {table: {'labels': [], 'table': []} for table in df_dict.keys()}\n",
    "\n",
    "    # First pass: Fit the HT objects for all tables\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        ht = ht_dict[table_name]\n",
    "        ht.fit(transformed_df_dict[table_name])  # Fit the HT object on the entire table\n",
    "\n",
    "    # Second pass: factorize primary keys and build the global primary key dictionary\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        primary_key = table_info['primary_key']\n",
    "        if primary_key is not None:  # Skip tables without a primary key\n",
    "            transformed_df_dict[table_name][primary_key], unique = pd.factorize(transformed_df_dict[table_name][primary_key])\n",
    "            primary_key_dict[table_name] = {primary_key: {key: value for value, key in enumerate(unique)}}\n",
    "            transform_method_dict[table_name]['labels'].append(primary_key)\n",
    "\n",
    "    # Third pass: replace foreign keys using the primary key dictionary\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        for parent_table, foreign_key in table_info['parent'].items():\n",
    "            primary_key = modelling_metadata[parent_table]['primary_key']\n",
    "            transformed_df_dict[table_name][foreign_key] = transformed_df_dict[table_name][foreign_key].map(primary_key_dict[parent_table][primary_key])\n",
    "            transform_method_dict[table_name]['labels'].append(foreign_key)\n",
    "\n",
    "    # Fourth pass: transform the remaining columns with the fitted HT objects\n",
    "    for table_name, table_info in modelling_metadata.items():\n",
    "        ht = ht_dict[table_name]\n",
    "        df = transformed_df_dict[table_name]\n",
    "        remaining_cols = [col for col in df.columns if col not in transform_method_dict[table_name]['labels']]\n",
    "        if remaining_cols:\n",
    "            df[remaining_cols] = ht.transform_subset(df[remaining_cols])  # Transform only the remaining columns\n",
    "            transform_method_dict[table_name]['table'].extend(remaining_cols)\n",
    "\n",
    "    return transformed_df_dict, transform_method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b17dc196-89c7-4b9f-81db-03be45252196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformed_data_collection, transform_method_dict = batch_transform(modelling_metadata, real_data_collection, formatter_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd532b97-d07d-4d17-b866-ed8c24e6a392",
   "metadata": {},
   "source": [
    "## Split Tables into Discrete and Continuous Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de5a1c05-f602-4318-b33e-63f33115d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(sdv_metadata, modelling_metadata):\n",
    "    # Define the discrete types.\n",
    "    discrete_sdtypes = ['categorical', 'id', 'boolean']\n",
    "\n",
    "    # Iterate over the metadata.\n",
    "    for df_name, content in sdv_metadata.items():\n",
    "        grouped_columns = {'discrete': [], 'continuous': []}\n",
    "        # Iterate over the columns of the DataFrame.\n",
    "        for column_name, col_type in content['columns'].items():\n",
    "            # If the column's 'sdtype' is one of the discrete types, add it to the 'discrete' group.\n",
    "            if col_type['sdtype'] in discrete_sdtypes:\n",
    "                grouped_columns['discrete'].append(column_name)\n",
    "            # elif col_type['sdtype'] == 'numerical':\n",
    "            #     if 'computer_representation' in col_type and 'Int' in col_type['computer_representation']:\n",
    "            #         grouped_columns['discrete'].append(column_name)\n",
    "            else:\n",
    "                # Otherwise, add it to the 'continuous' group.\n",
    "                grouped_columns['continuous'].append(column_name)\n",
    "        modelling_metadata[df_name]['numeric_types'] = grouped_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc1f7a13-f47f-4338-9171-878db4738413",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_columns(sdv_metadata.to_dict()['tables'], modelling_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cab2da9-db50-4db3-953d-cdc635c8bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_continuous_discrete(data_collection, modelling_metadata):\n",
    "    for df_name, content in modelling_metadata.items():\n",
    "        foreign_columns = []\n",
    "        label_columns = []\n",
    "        selected_columns = []\n",
    "        if content['parent'] != {}:\n",
    "            foreign_columns = list(modelling_metadata[df_name]['parent'].values())\n",
    "        discrete_columns = modelling_metadata[df_name]['numeric_types']['discrete']\n",
    "        if content['primary_key'] != None:\n",
    "            selected_columns = [col for col in discrete_columns if col != content['primary_key']]\n",
    "        selected_columns = list(set(foreign_columns) | set(selected_columns))\n",
    "        if selected_columns != [] or None:\n",
    "            modelling_metadata[df_name]['selected_columns'] = selected_columns\n",
    "        else:\n",
    "            modelling_metadata[df_name]['selected_columns'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f83047da-39a5-4831-8c52-76ab3dd5a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_continuous_discrete(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa2e80-2ca0-4d2c-881d-94de5c331119",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feb81dc5-44b4-4b54-9a6d-96a6b9fee048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_unique_elements(data):\n",
    "    unique_counts = data.nunique()\n",
    "    unique_dict = unique_counts.to_dict()\n",
    "    return unique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8846e997-fe62-44f4-9492-03ef527737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaling(df, min_=0, max_=1):\n",
    "    scaler = MinMaxScaler((min_,max_))\n",
    "    scaler = scaler.fit(df)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c29e184-ab47-4a05-9b1a-4f0ac24e8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_scaling(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(df)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0c8a567-b531-4b2c-b2ef-d50f188b9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_and_scaled_features(df, label_columns):\n",
    "    labels = df[label_columns]\n",
    "    table = df.drop(columns=label_columns)\n",
    "    table_column_names = table.columns\n",
    "    try:\n",
    "        scaled_table, table_scaler = normal_scaling(table)\n",
    "    except:\n",
    "        scaled_table = None\n",
    "        table_scaler = None\n",
    "    scaled_labels, labels_scaler = normal_scaling(labels)\n",
    "    return scaled_table, scaled_labels, table_scaler, labels_scaler, table_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c140cf3f-8d9e-4df9-b1cc-984d95a9806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_scaling(data_collection, modelling_metadata):\n",
    "    processed_data_collection = {}\n",
    "    backtransformation_dict = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        selected_columns = modelling_metadata[df_name]['selected_columns']\n",
    "        primary_key_column = modelling_metadata[df_name]['primary_key']\n",
    "        if primary_key_column != None:\n",
    "            label_columns = list(set([primary_key_column]) | set(selected_columns))\n",
    "        else:\n",
    "            label_columns = selected_columns\n",
    "            primary_key_scaler = None\n",
    "        table, labels, table_scaler, labels_scaler, table_columns = extract_key_and_scaled_features(df, label_columns)\n",
    "        processed_data_collection[df_name] = {'table':table,\n",
    "                                              'labels':labels}\n",
    "        backtransformation_dict[df_name] = {'table_scaler':table_scaler,\n",
    "                                            'labels_scaler':labels_scaler,\n",
    "                                       'table_col_names':table_columns,\n",
    "                                       'labels_col_names':label_columns,\n",
    "                                       'col_names':df.columns,\n",
    "                                           'unique_discrete_elements':n_unique_elements(df[selected_columns])}\n",
    "    return processed_data_collection, backtransformation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90c64e29-b4c5-4ea0-a6f3-3019fe5a7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_collection, backtransformation_dict = batch_scaling(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235ddc3-4ce2-4f3b-9ebc-d04abc9e35c8",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa1b97-50ad-4e78-898b-dcab7c854a4a",
   "metadata": {},
   "source": [
    "## Fit KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13fc578c-be40-411e-b779-01160cb65a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kde(data, kernel='tophat', bandwidth='scott', leaf_size=40):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth, leaf_size=leaf_size)\n",
    "    kde.fit(data)\n",
    "    unique_elements_dict = n_unique_elements(data)\n",
    "    return kde, unique_elements_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22b26696-67e0-4e28-a7b3-c927d67609e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_kde_fitting(data_collection, modelling_metadata):\n",
    "    kde_collection = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        selected_columns = modelling_metadata[df_name]['selected_columns']\n",
    "        if selected_columns != []:\n",
    "            kde, unique_elements = fit_kde(df[selected_columns])\n",
    "            kde_collection[df_name] = kde\n",
    "    return kde_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a08e6022-5106-4ef1-8559-10239f2bfe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_collection = batch_kde_fitting(transformed_data_collection, modelling_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49eb54-1738-4599-874d-571ca5fdd9be",
   "metadata": {},
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744d496-c6f4-4c88-a505-3b82acc3304b",
   "metadata": {},
   "source": [
    "### Create Custom Functions for WGAN Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dce64b6b-045a-4764-9118-be6959932044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(x_fake):\n",
    "    return -tf.reduce_mean(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4af23f37-66e7-4aaa-9c91-ec955911a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(x_real, x_fake):\n",
    "    real_loss = tf.reduce_mean(x_real)\n",
    "    fake_loss = tf.reduce_mean(x_fake)\n",
    "    return fake_loss - real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5ab2a8f-c18a-47b2-bbc3-7b9b3337b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(batch_size, real_table, fake_table, real_labels, discriminator):\n",
    "    alpha = np.random.normal(0, 1, [batch_size, real_table.shape[1]])\n",
    "    diff = fake_table - real_table\n",
    "    interpolated_table = real_table + alpha * diff\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated_table)\n",
    "        pred = discriminator([interpolated_table,real_labels], training=True)\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated_table])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64966e-d261-41f4-92b1-0d979c9deceb",
   "metadata": {},
   "source": [
    "### Define Plotting Function For G/D Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "558eca6b-dc6e-45a4-a77d-5a2139ef78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(logs):\n",
    "    n = len(logs)\n",
    "    n_rows = (n + 2) // 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (model_name, generator_log, discriminator_log) in enumerate(logs):\n",
    "        axes[i].plot(generator_log, label='gen')\n",
    "        axes[i].plot(discriminator_log, label='d')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(model_name)\n",
    "\n",
    "    for i in range(n, n_rows * 3):\n",
    "        axes[i].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134ba08-ad73-4d99-ae5d-a14e73c5a9d6",
   "metadata": {},
   "source": [
    "### Define Generator, discriminator and GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41167d3c-1509-42c7-bc7d-95694f11e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(table_dim, labels_dim, opt, loss, latent_dim=100):\n",
    "    noise_input = layers.Input(shape=(latent_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([noise_input, labels_input])\n",
    "    hidden_layer1 = layers.Dense(256,activation='relu',kernel_initializer='he_normal')(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    hidden_layer2 = layers.Dense(256,activation='relu',kernel_initializer='he_normal')(bn_layer1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    output_layer = layers.Dense(table_dim, activation='linear',kernel_initializer='glorot_normal')(bn_layer2)\n",
    "    generator = models.Model([noise_input, labels_input], output_layer)\n",
    "    generator.compile(optimizer=opt, loss=loss)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a95fea4d-2ef8-451c-99e1-9645b4f42429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(table_dim, labels_dim, opt, loss):\n",
    "    table_input = layers.Input(shape=(table_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([table_input, labels_input])\n",
    "    hidden_layer1 = layers.Dense(256,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal')(merge_input)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    dropout1 = layers.Dropout(0.5)(bn_layer1)\n",
    "    hidden_layer2 = layers.Dense(256,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal')(dropout1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    dropout2 = layers.Dropout(0.5)(bn_layer2)\n",
    "    output_layer = layers.Dense(1, activation='linear',kernel_initializer='glorot_normal')(dropout2)\n",
    "    discriminator = models.Model([table_input, labels_input], output_layer)\n",
    "    discriminator.compile(optimizer=opt, loss=loss)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cbc2d56-1baa-47de-8edf-aaf6b7ede7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_generator(table_dim, labels_dim, opt, loss, latent_dim=100):\n",
    "    noise_input = layers.Input(shape=(latent_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([noise_input, labels_input])\n",
    "    reshape_layer = layers.Reshape((1, -1))(merge_input)\n",
    "    hidden_layer1 = layers.Conv1DTranspose(32,kernel_size=7,activation='relu',kernel_initializer='he_normal',padding='same')(reshape_layer)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    hidden_layer2 = layers.Conv1DTranspose(64,kernel_size=7,activation='relu',kernel_initializer='he_normal',padding='same')(bn_layer1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    hidden_layer3 = layers.Conv1DTranspose(128,kernel_size=7,activation='relu',kernel_initializer='he_normal',padding='same')(bn_layer2)\n",
    "    bn_layer3 = layers.BatchNormalization()(hidden_layer3)\n",
    "    hidden_layer4 = layers.Conv1DTranspose(256,kernel_size=7,activation='relu',kernel_initializer='he_normal',padding='same')(bn_layer3)\n",
    "    bn_layer4 = layers.BatchNormalization()(hidden_layer4)\n",
    "    flatten_layer = layers.Flatten()(bn_layer4)\n",
    "    output_layer = layers.Dense(table_dim, activation='linear',kernel_initializer='glorot_normal')(flatten_layer)\n",
    "    generator = models.Model([noise_input, labels_input], output_layer)\n",
    "    generator.compile(optimizer=opt, loss=loss)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52d62f5a-2c81-424d-ad94-680d07d463c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_conv_discriminator(table_dim, labels_dim, opt, loss):\n",
    "    table_input = layers.Input(shape=(table_dim,))\n",
    "    labels_input = layers.Input(shape=(labels_dim,))\n",
    "    merge_input = layers.Concatenate()([table_input, labels_input])\n",
    "    reshape_layer = layers.Reshape((1, -1))(merge_input)\n",
    "    hidden_layer1 = layers.Conv1D(256,kernel_size=7,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(reshape_layer)\n",
    "    bn_layer1 = layers.BatchNormalization()(hidden_layer1)\n",
    "    dropout1 = layers.Dropout(0.2)(bn_layer1)\n",
    "    hidden_layer2 = layers.Conv1D(128,kernel_size=7,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout1)\n",
    "    bn_layer2 = layers.BatchNormalization()(hidden_layer2)\n",
    "    dropout2 = layers.Dropout(0.2)(bn_layer2)\n",
    "    hidden_layer3 = layers.Conv1D(64,kernel_size=7,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout2)\n",
    "    bn_layer3 = layers.BatchNormalization()(hidden_layer3)\n",
    "    dropout3 = layers.Dropout(0.2)(bn_layer3)\n",
    "    hidden_layer4 = layers.Conv1D(32,kernel_size=7,activation=layers.LeakyReLU(0.2),kernel_initializer='he_normal',padding='same')(dropout3)\n",
    "    bn_layer4 = layers.BatchNormalization()(hidden_layer4)\n",
    "    dropout4 = layers.Dropout(0.2)(bn_layer4)\n",
    "    flatten_layer = layers.Flatten()(dropout4)\n",
    "    output_layer = layers.Dense(1, activation='linear',kernel_initializer='glorot_normal')(flatten_layer)\n",
    "    discriminator = models.Model([table_input, labels_input], output_layer)\n",
    "    discriminator.compile(optimizer=opt, loss=loss)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2b8d7-5798-490f-bf13-8233ef51b8d3",
   "metadata": {},
   "source": [
    "### Define Training Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c39aa00-6c56-4ad7-97d3-e6c3d98cb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_gan(name, real_table, real_labels, n_discriminator=3, gp_weight=10, epochs=50, batch_size=400, latent_dim=100, seed=123, return_logs=True):\n",
    "    rand_seed = seed\n",
    "    half_batch = int(batch_size/2)\n",
    "    g_opt = optimizers.Adam()\n",
    "    c_opt = optimizers.Adam()\n",
    "    g_loss_func = generator_loss\n",
    "    d_loss_func = discriminator_loss\n",
    "    generator = build_generator(real_table.shape[1], real_labels.shape[1], opt=g_opt, loss=g_loss_func, latent_dim=latent_dim)\n",
    "    discriminator = build_discriminator(real_table.shape[1], real_labels.shape[1], opt=c_opt, loss=d_loss_func)\n",
    "    y_real = -1* np.ones((half_batch, 1))\n",
    "    y_fake = np.ones((half_batch, 1))\n",
    "    y_gan = np.ones((batch_size, 1))\n",
    "    \n",
    "    if return_logs:\n",
    "        generator_logs = []\n",
    "        discriminator_logs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(rand_seed)\n",
    "        start_time = time.time()\n",
    "        d_loss = []\n",
    "        \n",
    "        # train discriminator:\n",
    "        for _ in range(n_discriminator):\n",
    "            idx = np.random.randint(0, real_table.shape[0], half_batch)\n",
    "            x_real_labels = real_labels.iloc[idx]\n",
    "            x_real_table = real_table.iloc[idx]\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "                x_fake_table = generator([noise, x_real_labels], training=True)\n",
    "                d_logits_real = discriminator([x_real_table.values, x_real_labels], training=True)\n",
    "                d_logits_fake = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "                d_cost = d_loss_func(d_logits_real, d_logits_fake)\n",
    "                gp = gradient_penalty(half_batch, x_real_table.values, x_fake_table, x_real_labels.values, discriminator)\n",
    "                d_loss = d_cost + gp * gp_weight\n",
    "                rand_seed += 1\n",
    "                \n",
    "            c_grad = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            c_opt.apply_gradients(\n",
    "                zip(c_grad, discriminator.trainable_variables)\n",
    "            )\n",
    "            \n",
    "        d_loss = np.mean(d_loss)\n",
    "        # train generator/GAN\n",
    "        idx = np.random.randint(0, real_table.shape[0], batch_size)\n",
    "        x_real_labels = real_labels.iloc[idx]\n",
    "        x_gan = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_fake_table = generator([x_gan, x_real_labels], training=True)\n",
    "            g_logits = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "            g_loss = g_loss_func(g_logits)\n",
    "        g_grad = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(\n",
    "            zip(g_grad, generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        if return_logs:\n",
    "            generator_logs.append(g_loss)\n",
    "            discriminator_logs.append(d_loss)\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        it_per_s = 1 / elapsed_time\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch in [0,epochs-1]:\n",
    "            print(f\"[Epoch: {epoch+1}] [C loss: {np.round(d_loss, 3)}] [G loss: {np.round(g_loss, 3)}] [Speed: {round(it_per_s, 2)} it/s]\")\n",
    "        \n",
    "        rand_seed += 1\n",
    "        \n",
    "    generator.save('models_200/'+name+'_gen.h5')\n",
    "    \n",
    "    if return_logs:\n",
    "        return generator_logs, discriminator_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b2a9050-72d7-4ce0-bc58-4906bb319af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_sequential_gan(name, real_table, real_labels, n_discriminator=3, gp_weight=10, epochs=50, batch_size=400, latent_dim=100, seed=123, return_logs=True):\n",
    "    rand_seed = seed\n",
    "    half_batch = int(batch_size/2)\n",
    "    samp_size = min(batch_size, real_table.shape[0])\n",
    "    half_samp_size = min(half_batch, real_table.shape[0])\n",
    "    g_opt = optimizers.Adam()\n",
    "    c_opt = optimizers.Adam()\n",
    "    g_loss_func = generator_loss\n",
    "    d_loss_func = discriminator_loss\n",
    "    generator = build_conv_generator(real_table.shape[1], real_labels.shape[1], opt=g_opt, loss=g_loss_func, latent_dim=latent_dim)\n",
    "    discriminator = build_conv_discriminator(real_table.shape[1], real_labels.shape[1], opt=c_opt, loss=d_loss_func)\n",
    "    y_real = -1* np.ones((half_samp_size, 1))\n",
    "    y_fake = np.ones((half_samp_size, 1))\n",
    "    y_gan = np.ones((samp_size, 1))\n",
    "    \n",
    "    if return_logs:\n",
    "        generator_logs = []\n",
    "        discriminator_logs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(rand_seed)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train discriminator:\n",
    "        for _ in range(n_discriminator):\n",
    "            idx = np.random.randint(0, real_table.shape[0]-half_samp_size)\n",
    "            x_real_labels = real_labels.iloc[idx:(idx+half_samp_size)]\n",
    "            x_real_table = real_table.iloc[idx:(idx+half_samp_size)]\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = np.random.normal(0, 1, (half_samp_size, latent_dim))\n",
    "                x_fake_table = generator([noise, x_real_labels], training=True)\n",
    "                d_logits_real = discriminator([x_real_table.values, x_real_labels], training=True)\n",
    "                d_logits_fake = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "                d_cost = d_loss_func(d_logits_real, d_logits_fake)\n",
    "                gp = gradient_penalty(half_samp_size, x_real_table.values, x_fake_table, x_real_labels.values, discriminator)\n",
    "                d_loss = d_cost + gp * gp_weight\n",
    "                rand_seed += 1\n",
    "                \n",
    "            c_grad = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            c_opt.apply_gradients(\n",
    "                zip(c_grad, discriminator.trainable_variables)\n",
    "            )\n",
    "            \n",
    "        d_loss = np.mean(d_loss)\n",
    "        # train generator/GAN\n",
    "        idx = np.random.randint(0, real_table.shape[0]-samp_size)\n",
    "        x_real_labels = real_labels.iloc[idx:(idx+samp_size)]\n",
    "        x_gan = np.random.normal(0, 1, (samp_size, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_fake_table = generator([x_gan, x_real_labels], training=True)\n",
    "            g_logits = discriminator([x_fake_table, x_real_labels], training=True)\n",
    "            g_loss = g_loss_func(g_logits)\n",
    "        g_grad = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(\n",
    "            zip(g_grad, generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        if return_logs:\n",
    "            generator_logs.append(g_loss)\n",
    "            discriminator_logs.append(d_loss)\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        it_per_s = 1 / elapsed_time\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch in [0,epochs-1]:\n",
    "            print(f\"[Epoch: {epoch+1}] [C loss: {np.round(d_loss, 3)}] [G loss: {np.round(g_loss, 3)}] [Speed: {round(it_per_s, 2)} it/s]\")\n",
    "        \n",
    "        rand_seed += 1\n",
    "        \n",
    "    generator.save('models_200/'+name+'_gen.h5')\n",
    "    \n",
    "    if return_logs:\n",
    "        return generator_logs, discriminator_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ffe38c7-2d8e-4cfd-a7be-90b9f7804895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_training(processed_collection, modelling_metadata, training_parameter_dict):\n",
    "    logs = []\n",
    "    init_time = time.time()\n",
    "    for df_name, content in processed_collection.items():\n",
    "        start_time = time.time()\n",
    "        print(f'Learning \\\"{df_name}\\\" data......')\n",
    "        latent_dim = 100\n",
    "        if content['table'] is not None:\n",
    "            if modelling_metadata[df_name]['is_sequential'] == True:\n",
    "                generator_log, discriminator_log = train_individual_sequential_gan(df_name,\n",
    "                                                                            content['table'],\n",
    "                                                                            content['labels'],\n",
    "                                                                            epochs=training_parameter_dict[df_name]['epochs'],\n",
    "                                                                            latent_dim = latent_dim,\n",
    "                                                                            return_logs=True)\n",
    "            else:\n",
    "                generator_log, discriminator_log = train_individual_gan(df_name,\n",
    "                                                                 content['table'],\n",
    "                                                                 content['labels'],\n",
    "                                                                 epochs=training_parameter_dict[df_name]['epochs'],\n",
    "                                                                 latent_dim = latent_dim,\n",
    "                                                                 return_logs=True)\n",
    "            logs.append([df_name, generator_log, discriminator_log])\n",
    "            in_loop_elapsed_time = time.time() - start_time\n",
    "            minutes, seconds = divmod(in_loop_elapsed_time, 60)\n",
    "            print(\"Time Used: %dm %ds\" % (minutes, seconds))\n",
    "            print('.')\n",
    "            print('.')\n",
    "        else:\n",
    "            print(\"Skipped (fully-discrete table, fitted with KDE instead)\")\n",
    "            print('.')\n",
    "            print('.')\n",
    "    plot_history(logs)\n",
    "    total_elapsed_time = time.time() - init_time\n",
    "    minutes, seconds = divmod(total_elapsed_time, 60)\n",
    "    print(\"Total Time Used: %dm %ds\" % (minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30c126-718f-4fb8-9e1f-788856cb0409",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c01837b4-de65-4540-815d-f34f47ef61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epochs(data_collection, batch_size=400):\n",
    "    parameter_dict = {}\n",
    "    for df_name, df in data_collection.items():\n",
    "        epochs = len(df) / batch_size\n",
    "        if epochs < 10:\n",
    "            epochs = 10\n",
    "        elif epochs > 500:\n",
    "            epochs = 500\n",
    "        parameter_dict[df_name] = {'epochs':round(epochs)}\n",
    "    return parameter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e617bab8-b5d8-4dcd-9bd7-b171c311dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameter_dict = calculate_epochs(transformed_data_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76243ed7-ea8b-46f0-9de0-e9891795f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_parameter_dict = {'agency': {'epochs': 10},\n",
    "#                            'calendar': {'epochs': 10},\n",
    "#                            'calendar_dates': {'epochs': 10},\n",
    "#                            'routes': {'epochs': 10},\n",
    "#                            'stops': {'epochs': 10},\n",
    "#                            'stop_times': {'epochs': 10},\n",
    "#                            'trips': {'epochs': 10}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d933f9ca-b23d-4235-ba11-d769163605c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency': {'epochs': 10},\n",
       " 'calendar': {'epochs': 10},\n",
       " 'calendar_dates': {'epochs': 10},\n",
       " 'routes': {'epochs': 10},\n",
       " 'stops': {'epochs': 10},\n",
       " 'stop_times': {'epochs': 10},\n",
       " 'trips': {'epochs': 10}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_parameter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "460d4f25-0081-4232-a6c0-1e520da7a6de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning \"agency\" data......\n",
      "Skipped (fully-discrete table, fitted with KDE instead)\n",
      ".\n",
      ".\n",
      "Learning \"calendar\" data......\n",
      "[Epoch: 1] [C loss: 2.6679999828338623] [G loss: 0.04600000008940697] [Speed: 0.56 it/s]\n",
      "[Epoch: 10] [C loss: 2.677999973297119] [G loss: -0.017000000923871994] [Speed: 6.25 it/s]\n",
      "Time Used: 0m 4s\n",
      ".\n",
      ".\n",
      "Learning \"calendar_dates\" data......\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[59], line 10\u001b[0m, in \u001b[0;36mcollection_training\u001b[1;34m(processed_collection, modelling_metadata, training_parameter_dict)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modelling_metadata[df_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_sequential\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         generator_log, discriminator_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_individual_sequential_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_parameter_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mreturn_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m         generator_log, discriminator_log \u001b[38;5;241m=\u001b[39m train_individual_gan(df_name,\n\u001b[0;32m     18\u001b[0m                                                          content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     19\u001b[0m                                                          content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m                                                          epochs\u001b[38;5;241m=\u001b[39mtraining_parameter_dict[df_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m                                                          latent_dim \u001b[38;5;241m=\u001b[39m latent_dim,\n\u001b[0;32m     22\u001b[0m                                                          return_logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[58], line 26\u001b[0m, in \u001b[0;36mtrain_individual_sequential_gan\u001b[1;34m(name, real_table, real_labels, n_discriminator, gp_weight, epochs, batch_size, latent_dim, seed, return_logs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# train discriminator:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_discriminator):\n\u001b[1;32m---> 26\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mhalf_samp_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     x_real_labels \u001b[38;5;241m=\u001b[39m real_labels\u001b[38;5;241m.\u001b[39miloc[idx:(idx\u001b[38;5;241m+\u001b[39mhalf_samp_size)]\n\u001b[0;32m     28\u001b[0m     x_real_table \u001b[38;5;241m=\u001b[39m real_table\u001b[38;5;241m.\u001b[39miloc[idx:(idx\u001b[38;5;241m+\u001b[39mhalf_samp_size)]\n",
      "File \u001b[1;32mmtrand.pyx:746\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_bounded_integers.pyx:1338\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "collection_training(processed_data_collection, modelling_metadata, training_parameter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37613df5-a0f7-4158-90e9-39c2f683bb49",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3df09-2fa3-43b2-b3ff-7e20bb6e5a78",
   "metadata": {},
   "source": [
    "## Set the number of rows to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e39be14-abfa-422e-a33a-f6333dc191a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_dict = {k:{'nrows':len(v['labels'])} for k,v in processed_data_collection.items()}\n",
    "generation_dict['calendar_dates']['sort_by'] = ['service_id','date']\n",
    "generation_dict['stop_times']['sort_by'] = ['trip_id','stop_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26712e2e-c0a2-4132-af93-68af04c1adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency': {'nrows': 15},\n",
       " 'calendar': {'nrows': 100},\n",
       " 'calendar_dates': {'nrows': 100, 'sort_by': ['service_id', 'date']},\n",
       " 'routes': {'nrows': 100},\n",
       " 'stops': {'nrows': 100},\n",
       " 'stop_times': {'nrows': 50, 'sort_by': ['trip_id', 'stop_sequence']},\n",
       " 'trips': {'nrows': 100}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a43cfc-9f13-4762-b54d-56d1fd69ee0c",
   "metadata": {},
   "source": [
    "## Define Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e155a03a-d4ff-440c-bb49-e1b15a153097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_unique_elements(data, unique_elements, ignore=[]):\n",
    "    for col, n_unique in unique_elements.items():\n",
    "        if col not in ignore:\n",
    "            data[col] = np.round(data[col]).astype(int)\n",
    "            min_val, max_val = data[col].min(), data[col].max()\n",
    "            original_range = np.linspace(min_val, max_val, n_unique)\n",
    "            new_range = np.arange(n_unique)\n",
    "            # interp_func = interp1d(original_range, new_range, kind='linear', bounds_error=False, fill_value=(0, n_unique - 1))\n",
    "            interp_func = interp1d(original_range, new_range, kind='nearest', bounds_error=False, fill_value=(0, n_unique - 1))\n",
    "            data[col] = interp_func(data[col]).astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f65d8-b4b5-44cd-b5ed-65e355702a3d",
   "metadata": {},
   "source": [
    "## Define KDE Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f189dabd-19bf-4258-b1c0-ac076242a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(kde, n_samples, col_names, round_=True):\n",
    "    new_data = kde.sample(n_samples)\n",
    "    if round_:\n",
    "        new_data_df = pd.DataFrame(new_data, columns=col_names).round().astype(int)\n",
    "    else:\n",
    "        new_data_df = pd.DataFrame(new_data, columns=col_names)\n",
    "    return new_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771edf13-94eb-404e-b297-d967d61553d1",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7926ee0f-cac2-4ea9-9852-a36b9d86daa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_synth_data_collection(kde_collection, modelling_metadata, generation_dict, backtransformation_dict, formatter_collection, transform_method_dict, latent_dim=100, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    synthetic_data_collection = {}\n",
    "    custom_objects = {'generator_loss':generator_loss}\n",
    "    for df_name, content in modelling_metadata.items():\n",
    "        if len(backtransformation_dict[df_name]['table_col_names']) != 0:\n",
    "            generator = models.load_model('models_200/'+df_name+'_gen.h5', custom_objects=custom_objects)\n",
    "        else:\n",
    "            generator = None\n",
    "        primary_key = content['primary_key']\n",
    "        child = content['child']\n",
    "        parent = content['parent']\n",
    "        n_samples = generation_dict[df_name]['nrows']\n",
    "        \n",
    "        fake_primary_key = None\n",
    "        fake_selected_columns = None\n",
    "        if primary_key != None:\n",
    "            fake_primary_key = np.arange(n_samples)\n",
    "            fake_primary_key = pd.DataFrame({primary_key:fake_primary_key})\n",
    "            \n",
    "        if content['selected_columns'] != []:\n",
    "            kde = kde_collection[df_name]\n",
    "            fake_selected_columns = generate_labels(kde, n_samples, content['selected_columns'])\n",
    "            unique_elements = backtransformation_dict[df_name]['unique_discrete_elements']\n",
    "            if parent != None:\n",
    "                ignore = []\n",
    "                for _, f_keys in parent.items():\n",
    "                    ignore.append(f_keys)\n",
    "                fake_selected_columns = adjust_unique_elements(fake_selected_columns, unique_elements, ignore=ignore)\n",
    "            else:\n",
    "                fake_selected_columns = adjust_unique_elements(fake_selected_columns, unique_elements)\n",
    "        \n",
    "        if fake_primary_key is not None:\n",
    "            if fake_selected_columns is not None:\n",
    "                fake_labels = pd.concat([fake_primary_key, fake_selected_columns], axis=1)\n",
    "            else:\n",
    "                fake_labels = fake_primary_key\n",
    "        else:\n",
    "            fake_labels = fake_selected_columns\n",
    "        \n",
    "        fake_labels = fake_labels.reindex(columns=backtransformation_dict[df_name]['labels_col_names'])\n",
    "        if content['is_sequential'] == True:\n",
    "            fake_labels = fake_labels.sort_values(by=content['sort_order'])\n",
    "            \n",
    "        scaled_fake_labels = backtransformation_dict[df_name]['labels_scaler'].transform(fake_labels)\n",
    "        \n",
    "        if generator is not None:\n",
    "            latent_dim = 100\n",
    "            noise = np.random.normal(0, 1, size=(n_samples, latent_dim))\n",
    "            raw_fake_table = generator.predict([noise, scaled_fake_labels])\n",
    "            fake_table = backtransformation_dict[df_name]['table_scaler'].inverse_transform(raw_fake_table)\n",
    "            fake_table = pd.DataFrame(fake_table, columns=backtransformation_dict[df_name]['table_col_names'])\n",
    "            fake_data = pd.concat([fake_labels, fake_table], axis=1)\n",
    "            fake_subset = formatter_collection[df_name].reverse_transform_subset(fake_data[transform_method_dict[df_name]['table']])\n",
    "            fake_data = pd.concat([fake_data[transform_method_dict[df_name]['labels']], fake_subset], axis=1)\n",
    "        else:\n",
    "            fake_data = fake_labels\n",
    "            fake_subset = formatter_collection[df_name].reverse_transform_subset(fake_data[transform_method_dict[df_name]['table']])\n",
    "            fake_data = pd.concat([fake_data[transform_method_dict[df_name]['labels']], fake_subset], axis=1)\n",
    "        fake_data = fake_data.reindex(columns=backtransformation_dict[df_name]['col_names'])\n",
    "        synthetic_data_collection[df_name] = fake_data\n",
    "        \n",
    "    return synthetic_data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1af7da14-2d7d-4c16-80e0-5c51f93f8675",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.53 s\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "synthetic_data_collection = generate_synth_data_collection(kde_collection, modelling_metadata, generation_dict, backtransformation_dict, formatter_collection, transform_method_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32de55-988b-4271-9750-d5281462c68f",
   "metadata": {},
   "source": [
    "## Generate/Replace Key Columns with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56f7de02-e103-4384-b8ff-dc396dc8337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keys_with_regex(dataframes, metadata):\n",
    "    # Generate a mapping from old primary keys to new ones for each table\n",
    "    primary_key_mappings = {}\n",
    "    for table_name, table_info in metadata['tables'].items():\n",
    "        df = dataframes[table_name]\n",
    "\n",
    "        # Skip tables without a primary key\n",
    "        if 'primary_key' not in table_info:\n",
    "            continue\n",
    "\n",
    "        primary_key = table_info['primary_key']\n",
    "\n",
    "        # Skip columns without a regex_format\n",
    "        if 'regex_format' not in table_info['columns'][primary_key]:\n",
    "            continue\n",
    "\n",
    "        regex = table_info['columns'][primary_key]['regex_format']\n",
    "\n",
    "        # Generate new primary keys\n",
    "        new_primary_keys = [exrex.getone(regex) for _ in range(len(df))]\n",
    "\n",
    "        # Check if new keys are unique. If not, regenerate.\n",
    "        while len(new_primary_keys) != len(set(new_primary_keys)):\n",
    "            new_primary_keys = [exrex.getone(regex) for _ in range(len(df))]\n",
    "\n",
    "        # Create a mapping from old primary keys to new ones\n",
    "        primary_key_mappings[table_name] = dict(zip(df[primary_key], new_primary_keys))\n",
    "\n",
    "        # Replace the primary keys in the original dataframe\n",
    "        df[primary_key] = new_primary_keys\n",
    "\n",
    "    # Update the foreign keys in each table\n",
    "    for relationship in metadata['relationships']:\n",
    "        parent_table_name = relationship['parent_table_name']\n",
    "        child_table_name = relationship['child_table_name']\n",
    "        child_foreign_key = relationship['child_foreign_key']\n",
    "\n",
    "        # Get the mapping from old to new primary keys for the parent table\n",
    "        key_mapping = primary_key_mappings.get(parent_table_name)\n",
    "\n",
    "        # If there's no key mapping (i.e., the parent table has no primary key),\n",
    "        # skip this relationship\n",
    "        if key_mapping is None:\n",
    "            continue\n",
    "\n",
    "        # Replace the foreign keys in the child table\n",
    "        dataframes[child_table_name][child_foreign_key] = dataframes[child_table_name][child_foreign_key].map(key_mapping)\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f7bbba1-5552-4c7c-82f9-c22d64c0d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_collection = replace_keys_with_regex(synthetic_data_collection, sdv_metadata.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dad6d-b2f2-4505-97c6-538b676f81c4",
   "metadata": {},
   "source": [
    "## Adjust Daylight-Savings Delta-Time to Traditional DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "515a7451-06e9-48ad-b9a4-f6d39e8bc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_collection['stop_times'] = adjust_time(synthetic_data_collection['stop_times'], ['arrival_time','departure_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da7938-19e7-45a9-8b48-21c196f3c86b",
   "metadata": {},
   "source": [
    "# Save Generated Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e640674b-5d1e-42e9-a9d8-52cf78c9c87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('pkl/gtfs200/real_data_collection.pkl', 'wb') as f:\n",
    "    pickle.dump(real_data_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e622f869-9a16-4878-832c-2ea5f899e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl/gtfs200/synthetic_data_full_epoch.pkl', 'wb') as f:\n",
    "    pickle.dump(synthetic_data_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "469095f2-644f-4207-98b5-e56d3b032162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pkl/gtfs200/synthetic_data_10epoch.pkl', 'wb') as f:\n",
    "#     pickle.dump(synthetic_data_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f0559f9-d4b8-4244-98da-7877c32a72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl/gtfs200/sdv_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(sdv_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3303b-df2d-46ba-991a-2ad74a7d3220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
